
==> Audit <==
|------------|----------------|----------|------------|---------|---------------------|---------------------|
|  Command   |      Args      | Profile  |    User    | Version |     Start Time      |      End Time       |
|------------|----------------|----------|------------|---------|---------------------|---------------------|
| start      |                | minikube | kunaldange | v1.34.0 | 07 Nov 24 22:14 IST |                     |
| start      |                | minikube | kunaldange | v1.34.0 | 07 Nov 24 22:16 IST |                     |
| start      |                | minikube | kunaldange | v1.34.0 | 07 Nov 24 22:20 IST |                     |
| start      |                | minikube | kunaldange | v1.35.0 | 05 Mar 25 19:43 IST |                     |
| start      |                | minikube | kunaldange | v1.35.0 | 05 Mar 25 19:45 IST | 05 Mar 25 19:49 IST |
| dashboard  |                | minikube | kunaldange | v1.35.0 | 05 Mar 25 19:50 IST |                     |
| service    | hello-minikube | minikube | kunaldange | v1.35.0 | 05 Mar 25 19:55 IST |                     |
| pause      |                | minikube | kunaldange | v1.35.0 | 05 Mar 25 19:56 IST | 05 Mar 25 19:56 IST |
| stop       |                | minikube | kunaldange | v1.35.0 | 05 Mar 25 19:56 IST | 05 Mar 25 19:56 IST |
| start      |                | minikube | kunaldange | v1.35.0 | 06 Mar 25 21:30 IST |                     |
| start      |                | minikube | kunaldange | v1.35.0 | 06 Mar 25 21:31 IST | 06 Mar 25 21:31 IST |
| docker-env |                | minikube | kunaldange | v1.35.0 | 06 Mar 25 21:36 IST | 06 Mar 25 21:36 IST |
| ip         |                | minikube | kunaldange | v1.35.0 | 06 Mar 25 23:29 IST |                     |
| start      |                | minikube | kunaldange | v1.35.0 | 07 Mar 25 10:36 IST | 07 Mar 25 10:37 IST |
| ip         |                | minikube | kunaldange | v1.35.0 | 07 Mar 25 10:37 IST | 07 Mar 25 10:37 IST |
| ip         |                | minikube | kunaldange | v1.35.0 | 07 Mar 25 12:27 IST | 07 Mar 25 12:27 IST |
| service    | webapp-service | minikube | kunaldange | v1.35.0 | 07 Mar 25 12:29 IST |                     |
| ip         |                | minikube | kunaldange | v1.35.0 | 07 Mar 25 12:37 IST | 07 Mar 25 12:37 IST |
| service    | webapp-service | minikube | kunaldange | v1.35.0 | 07 Mar 25 12:41 IST |                     |
|------------|----------------|----------|------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/03/07 10:36:41
Running on machine: Kunals-MacBook-Air
Binary: Built with gc go1.23.4 for darwin/arm64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0307 10:36:41.926768   35916 out.go:345] Setting OutFile to fd 1 ...
I0307 10:36:41.927251   35916 out.go:397] isatty.IsTerminal(1) = true
I0307 10:36:41.927254   35916 out.go:358] Setting ErrFile to fd 2...
I0307 10:36:41.927259   35916 out.go:397] isatty.IsTerminal(2) = true
I0307 10:36:41.927872   35916 root.go:338] Updating PATH: /Users/kunaldange/.minikube/bin
W0307 10:36:41.928443   35916 root.go:314] Error reading config file at /Users/kunaldange/.minikube/config/config.json: open /Users/kunaldange/.minikube/config/config.json: no such file or directory
I0307 10:36:41.929484   35916 out.go:352] Setting JSON to false
I0307 10:36:41.974213   35916 start.go:129] hostinfo: {"hostname":"Kunals-MacBook-Air.local","uptime":1675237,"bootTime":1739648764,"procs":460,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"15.1.1","kernelVersion":"24.1.0","kernelArch":"arm64","virtualizationSystem":"","virtualizationRole":"","hostId":"8ca198d1-74c1-5e1e-997f-7da32e4a0b07"}
W0307 10:36:41.974350   35916 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0307 10:36:41.979937   35916 out.go:177] 😄  minikube v1.35.0 on Darwin 15.1.1 (arm64)
I0307 10:36:41.987213   35916 notify.go:220] Checking for updates...
I0307 10:36:41.987388   35916 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0307 10:36:41.987913   35916 driver.go:394] Setting default libvirt URI to qemu:///system
I0307 10:36:42.026970   35916 docker.go:123] docker version: linux-27.5.1:Docker Desktop 4.38.0 (181591)
I0307 10:36:42.027909   35916 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0307 10:36:42.491896   35916 info.go:266] docker info: {ID:2f3c69c9-86e4-4f8e-b0bb-d5a5e6dc97aa Containers:7 ContainersRunning:1 ContainersPaused:0 ContainersStopped:6 Images:8 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:69 OomKillDisable:false NGoroutines:107 SystemTime:2025-03-07 05:06:42.48006875 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:18 KernelVersion:6.12.5-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:4109344768 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///Users/kunaldange/Library/Containers/com.docker.docker/Data/docker-cli.sock] ExperimentalBuild:false ServerVersion:27.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb Expected:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb} RuncCommit:{ID:v1.1.12-0-g51d5e946 Expected:v1.1.12-0-g51d5e946} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:/Users/kunaldange/.docker/cli-plugins/docker-ai SchemaVersion:0.1.0 ShortDescription:Ask Gordon - Docker Agent Vendor:Docker Inc. Version:v0.7.3] map[Name:buildx Path:/Users/kunaldange/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.20.1-desktop.2] map[Name:compose Path:/Users/kunaldange/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.32.4-desktop.1] map[Name:debug Path:/Users/kunaldange/.docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:/Users/kunaldange/.docker/cli-plugins/docker-desktop SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.4] map[Name:dev Path:/Users/kunaldange/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/Users/kunaldange/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:feedback Path:/Users/kunaldange/.docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:/Users/kunaldange/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:/Users/kunaldange/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/Users/kunaldange/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.16.1]] Warnings:<nil>}}
I0307 10:36:42.496835   35916 out.go:177] ✨  Using the docker driver based on existing profile
I0307 10:36:42.504909   35916 start.go:297] selected driver: docker
I0307 10:36:42.504955   35916 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0307 10:36:42.505029   35916 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0307 10:36:42.505186   35916 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0307 10:36:42.612269   35916 info.go:266] docker info: {ID:2f3c69c9-86e4-4f8e-b0bb-d5a5e6dc97aa Containers:7 ContainersRunning:1 ContainersPaused:0 ContainersStopped:6 Images:8 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:67 OomKillDisable:false NGoroutines:102 SystemTime:2025-03-07 05:06:42.604181584 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:18 KernelVersion:6.12.5-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:4109344768 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///Users/kunaldange/Library/Containers/com.docker.docker/Data/docker-cli.sock] ExperimentalBuild:false ServerVersion:27.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb Expected:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb} RuncCommit:{ID:v1.1.12-0-g51d5e946 Expected:v1.1.12-0-g51d5e946} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:/Users/kunaldange/.docker/cli-plugins/docker-ai SchemaVersion:0.1.0 ShortDescription:Ask Gordon - Docker Agent Vendor:Docker Inc. Version:v0.7.3] map[Name:buildx Path:/Users/kunaldange/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.20.1-desktop.2] map[Name:compose Path:/Users/kunaldange/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.32.4-desktop.1] map[Name:debug Path:/Users/kunaldange/.docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:/Users/kunaldange/.docker/cli-plugins/docker-desktop SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.4] map[Name:dev Path:/Users/kunaldange/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/Users/kunaldange/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:feedback Path:/Users/kunaldange/.docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:/Users/kunaldange/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:/Users/kunaldange/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/Users/kunaldange/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.16.1]] Warnings:<nil>}}
I0307 10:36:42.620426   35916 cni.go:84] Creating CNI manager for ""
I0307 10:36:42.620981   35916 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0307 10:36:42.621229   35916 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0307 10:36:42.629951   35916 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I0307 10:36:42.633946   35916 cache.go:121] Beginning downloading kic base image for docker with docker
I0307 10:36:42.637845   35916 out.go:177] 🚜  Pulling base image v0.0.46 ...
I0307 10:36:42.645906   35916 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0307 10:36:42.645931   35916 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon
I0307 10:36:42.645976   35916 preload.go:146] Found local preload: /Users/kunaldange/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-arm64.tar.lz4
I0307 10:36:42.645982   35916 cache.go:56] Caching tarball of preloaded images
I0307 10:36:42.646783   35916 preload.go:172] Found /Users/kunaldange/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-arm64.tar.lz4 in cache, skipping download
I0307 10:36:42.646825   35916 cache.go:59] Finished verifying existence of preloaded tar for v1.32.0 on docker
I0307 10:36:42.646978   35916 profile.go:143] Saving config to /Users/kunaldange/.minikube/profiles/minikube/config.json ...
I0307 10:36:42.734461   35916 cache.go:150] Downloading gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 to local cache
I0307 10:36:42.735478   35916 image.go:65] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local cache directory
I0307 10:36:42.735520   35916 image.go:68] Found gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local cache directory, skipping pull
I0307 10:36:42.735523   35916 image.go:137] gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 exists in cache, skipping pull
I0307 10:36:42.735532   35916 cache.go:153] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 as a tarball
I0307 10:36:42.735535   35916 cache.go:163] Loading gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 from local cache
I0307 10:36:48.804413   35916 cache.go:165] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 from cached tarball
I0307 10:36:48.804945   35916 cache.go:227] Successfully downloaded all kic artifacts
I0307 10:36:48.806370   35916 start.go:360] acquireMachinesLock for minikube: {Name:mkf854627247a194d973b654402c2213c9ee11a4 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0307 10:36:48.809391   35916 start.go:364] duration metric: took 2.845292ms to acquireMachinesLock for "minikube"
I0307 10:36:48.809508   35916 start.go:96] Skipping create...Using existing machine configuration
I0307 10:36:48.809516   35916 fix.go:54] fixHost starting: 
I0307 10:36:48.810378   35916 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0307 10:36:48.842167   35916 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0307 10:36:48.842214   35916 fix.go:138] unexpected machine state, will restart: <nil>
I0307 10:36:48.848450   35916 out.go:177] 🔄  Restarting existing docker container for "minikube" ...
I0307 10:36:48.855774   35916 cli_runner.go:164] Run: docker start minikube
I0307 10:36:49.088037   35916 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0307 10:36:49.121192   35916 kic.go:430] container "minikube" state is running.
I0307 10:36:49.122004   35916 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0307 10:36:49.146953   35916 profile.go:143] Saving config to /Users/kunaldange/.minikube/profiles/minikube/config.json ...
I0307 10:36:49.147447   35916 machine.go:93] provisionDockerMachine start ...
I0307 10:36:49.147517   35916 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0307 10:36:49.172286   35916 main.go:141] libmachine: Using SSH client type: native
I0307 10:36:49.173908   35916 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1050c4680] 0x1050c6ec0 <nil>  [] 0s} 127.0.0.1 51957 <nil> <nil>}
I0307 10:36:49.173917   35916 main.go:141] libmachine: About to run SSH command:
hostname
I0307 10:36:49.175320   35916 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0307 10:36:52.325242   35916 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0307 10:36:52.325594   35916 ubuntu.go:169] provisioning hostname "minikube"
I0307 10:36:52.326021   35916 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0307 10:36:52.373963   35916 main.go:141] libmachine: Using SSH client type: native
I0307 10:36:52.374497   35916 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1050c4680] 0x1050c6ec0 <nil>  [] 0s} 127.0.0.1 51957 <nil> <nil>}
I0307 10:36:52.374506   35916 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0307 10:36:52.531893   35916 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0307 10:36:52.532114   35916 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0307 10:36:52.577804   35916 main.go:141] libmachine: Using SSH client type: native
I0307 10:36:52.578121   35916 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1050c4680] 0x1050c6ec0 <nil>  [] 0s} 127.0.0.1 51957 <nil> <nil>}
I0307 10:36:52.578135   35916 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0307 10:36:52.702059   35916 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0307 10:36:52.702116   35916 ubuntu.go:175] set auth options {CertDir:/Users/kunaldange/.minikube CaCertPath:/Users/kunaldange/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/kunaldange/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/kunaldange/.minikube/machines/server.pem ServerKeyPath:/Users/kunaldange/.minikube/machines/server-key.pem ClientKeyPath:/Users/kunaldange/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/kunaldange/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/kunaldange/.minikube}
I0307 10:36:52.702186   35916 ubuntu.go:177] setting up certificates
I0307 10:36:52.702217   35916 provision.go:84] configureAuth start
I0307 10:36:52.702508   35916 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0307 10:36:52.746469   35916 provision.go:143] copyHostCerts
I0307 10:36:52.747527   35916 exec_runner.go:144] found /Users/kunaldange/.minikube/ca.pem, removing ...
I0307 10:36:52.747767   35916 exec_runner.go:203] rm: /Users/kunaldange/.minikube/ca.pem
I0307 10:36:52.748072   35916 exec_runner.go:151] cp: /Users/kunaldange/.minikube/certs/ca.pem --> /Users/kunaldange/.minikube/ca.pem (1086 bytes)
I0307 10:36:52.748632   35916 exec_runner.go:144] found /Users/kunaldange/.minikube/cert.pem, removing ...
I0307 10:36:52.748636   35916 exec_runner.go:203] rm: /Users/kunaldange/.minikube/cert.pem
I0307 10:36:52.748766   35916 exec_runner.go:151] cp: /Users/kunaldange/.minikube/certs/cert.pem --> /Users/kunaldange/.minikube/cert.pem (1131 bytes)
I0307 10:36:52.749261   35916 exec_runner.go:144] found /Users/kunaldange/.minikube/key.pem, removing ...
I0307 10:36:52.749267   35916 exec_runner.go:203] rm: /Users/kunaldange/.minikube/key.pem
I0307 10:36:52.749542   35916 exec_runner.go:151] cp: /Users/kunaldange/.minikube/certs/key.pem --> /Users/kunaldange/.minikube/key.pem (1675 bytes)
I0307 10:36:52.749896   35916 provision.go:117] generating server cert: /Users/kunaldange/.minikube/machines/server.pem ca-key=/Users/kunaldange/.minikube/certs/ca.pem private-key=/Users/kunaldange/.minikube/certs/ca-key.pem org=kunaldange.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0307 10:36:52.924435   35916 provision.go:177] copyRemoteCerts
I0307 10:36:52.924938   35916 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0307 10:36:52.925001   35916 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0307 10:36:52.949800   35916 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51957 SSHKeyPath:/Users/kunaldange/.minikube/machines/minikube/id_rsa Username:docker}
I0307 10:36:53.048558   35916 ssh_runner.go:362] scp /Users/kunaldange/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1086 bytes)
I0307 10:36:53.069749   35916 ssh_runner.go:362] scp /Users/kunaldange/.minikube/machines/server.pem --> /etc/docker/server.pem (1188 bytes)
I0307 10:36:53.087198   35916 ssh_runner.go:362] scp /Users/kunaldange/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0307 10:36:53.102608   35916 provision.go:87] duration metric: took 400.10625ms to configureAuth
I0307 10:36:53.102623   35916 ubuntu.go:193] setting minikube options for container-runtime
I0307 10:36:53.102945   35916 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0307 10:36:53.103026   35916 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0307 10:36:53.127767   35916 main.go:141] libmachine: Using SSH client type: native
I0307 10:36:53.127960   35916 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1050c4680] 0x1050c6ec0 <nil>  [] 0s} 127.0.0.1 51957 <nil> <nil>}
I0307 10:36:53.127965   35916 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0307 10:36:53.245498   35916 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0307 10:36:53.245516   35916 ubuntu.go:71] root file system type: overlay
I0307 10:36:53.245790   35916 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0307 10:36:53.246031   35916 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0307 10:36:53.287285   35916 main.go:141] libmachine: Using SSH client type: native
I0307 10:36:53.287519   35916 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1050c4680] 0x1050c6ec0 <nil>  [] 0s} 127.0.0.1 51957 <nil> <nil>}
I0307 10:36:53.287577   35916 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0307 10:36:53.410254   35916 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0307 10:36:53.410436   35916 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0307 10:36:53.451648   35916 main.go:141] libmachine: Using SSH client type: native
I0307 10:36:53.451925   35916 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1050c4680] 0x1050c6ec0 <nil>  [] 0s} 127.0.0.1 51957 <nil> <nil>}
I0307 10:36:53.451939   35916 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0307 10:36:53.577318   35916 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0307 10:36:53.577337   35916 machine.go:96] duration metric: took 4.430033416s to provisionDockerMachine
I0307 10:36:53.577350   35916 start.go:293] postStartSetup for "minikube" (driver="docker")
I0307 10:36:53.577362   35916 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0307 10:36:53.577510   35916 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0307 10:36:53.577586   35916 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0307 10:36:53.609860   35916 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51957 SSHKeyPath:/Users/kunaldange/.minikube/machines/minikube/id_rsa Username:docker}
I0307 10:36:53.695072   35916 ssh_runner.go:195] Run: cat /etc/os-release
I0307 10:36:53.697725   35916 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0307 10:36:53.697760   35916 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0307 10:36:53.697771   35916 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0307 10:36:53.697778   35916 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0307 10:36:53.697786   35916 filesync.go:126] Scanning /Users/kunaldange/.minikube/addons for local assets ...
I0307 10:36:53.697967   35916 filesync.go:126] Scanning /Users/kunaldange/.minikube/files for local assets ...
I0307 10:36:53.698066   35916 start.go:296] duration metric: took 120.711042ms for postStartSetup
I0307 10:36:53.698405   35916 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0307 10:36:53.698558   35916 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0307 10:36:53.733428   35916 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51957 SSHKeyPath:/Users/kunaldange/.minikube/machines/minikube/id_rsa Username:docker}
I0307 10:36:53.817611   35916 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0307 10:36:53.820809   35916 fix.go:56] duration metric: took 5.0114615s for fixHost
I0307 10:36:53.820831   35916 start.go:83] releasing machines lock for "minikube", held for 5.011581958s
I0307 10:36:53.820944   35916 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0307 10:36:53.860324   35916 ssh_runner.go:195] Run: cat /version.json
I0307 10:36:53.860418   35916 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0307 10:36:53.861406   35916 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0307 10:36:53.861666   35916 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0307 10:36:53.891297   35916 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51957 SSHKeyPath:/Users/kunaldange/.minikube/machines/minikube/id_rsa Username:docker}
I0307 10:36:53.891434   35916 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51957 SSHKeyPath:/Users/kunaldange/.minikube/machines/minikube/id_rsa Username:docker}
I0307 10:36:54.250493   35916 ssh_runner.go:195] Run: systemctl --version
I0307 10:36:54.257435   35916 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0307 10:36:54.261023   35916 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0307 10:36:54.274231   35916 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0307 10:36:54.274524   35916 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0307 10:36:54.280478   35916 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0307 10:36:54.280527   35916 start.go:495] detecting cgroup driver to use...
I0307 10:36:54.280570   35916 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0307 10:36:54.282033   35916 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0307 10:36:54.292096   35916 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0307 10:36:54.298634   35916 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0307 10:36:54.304584   35916 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0307 10:36:54.304743   35916 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0307 10:36:54.311005   35916 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0307 10:36:54.316953   35916 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0307 10:36:54.322853   35916 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0307 10:36:54.328597   35916 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0307 10:36:54.334005   35916 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0307 10:36:54.339774   35916 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0307 10:36:54.345834   35916 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0307 10:36:54.351823   35916 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0307 10:36:54.358285   35916 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0307 10:36:54.363397   35916 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0307 10:36:54.410007   35916 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0307 10:36:54.490559   35916 start.go:495] detecting cgroup driver to use...
I0307 10:36:54.490584   35916 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0307 10:36:54.491039   35916 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0307 10:36:54.500238   35916 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0307 10:36:54.500391   35916 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0307 10:36:54.510571   35916 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0307 10:36:54.522732   35916 ssh_runner.go:195] Run: which cri-dockerd
I0307 10:36:54.525624   35916 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0307 10:36:54.531271   35916 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0307 10:36:54.543754   35916 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0307 10:36:54.623087   35916 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0307 10:36:54.671419   35916 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0307 10:36:54.671607   35916 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0307 10:36:54.682868   35916 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0307 10:36:54.726992   35916 ssh_runner.go:195] Run: sudo systemctl restart docker
I0307 10:36:56.536688   35916 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.809730875s)
I0307 10:36:56.536890   35916 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0307 10:36:56.547994   35916 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0307 10:36:56.560695   35916 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0307 10:36:56.569810   35916 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0307 10:36:56.620423   35916 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0307 10:36:56.669149   35916 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0307 10:36:56.718166   35916 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0307 10:36:56.756939   35916 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0307 10:36:56.764916   35916 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0307 10:36:56.813157   35916 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0307 10:36:57.011252   35916 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0307 10:36:57.012110   35916 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0307 10:36:57.014808   35916 start.go:563] Will wait 60s for crictl version
I0307 10:36:57.014915   35916 ssh_runner.go:195] Run: which crictl
I0307 10:36:57.017340   35916 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0307 10:36:57.106043   35916 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.4.1
RuntimeApiVersion:  v1
I0307 10:36:57.106166   35916 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0307 10:36:57.181448   35916 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0307 10:36:57.203372   35916 out.go:235] 🐳  Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
I0307 10:36:57.204411   35916 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0307 10:36:57.323259   35916 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0307 10:36:57.324170   35916 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0307 10:36:57.327243   35916 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0307 10:36:57.334496   35916 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0307 10:36:57.374036   35916 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0307 10:36:57.374164   35916 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0307 10:36:57.374229   35916 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0307 10:36:57.388487   35916 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
kubernetesui/dashboard:<none>
kicbase/echo-server:1.0
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0307 10:36:57.388495   35916 docker.go:619] Images already preloaded, skipping extraction
I0307 10:36:57.388801   35916 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0307 10:36:57.404205   35916 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
kubernetesui/dashboard:<none>
kicbase/echo-server:1.0
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0307 10:36:57.404219   35916 cache_images.go:84] Images are preloaded, skipping loading
I0307 10:36:57.404232   35916 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.32.0 docker true true} ...
I0307 10:36:57.404351   35916 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.32.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0307 10:36:57.404432   35916 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0307 10:36:57.556139   35916 cni.go:84] Creating CNI manager for ""
I0307 10:36:57.556160   35916 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0307 10:36:57.556467   35916 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0307 10:36:57.556496   35916 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.32.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0307 10:36:57.557107   35916 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.32.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0307 10:36:57.557520   35916 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.32.0
I0307 10:36:57.564306   35916 binaries.go:44] Found k8s binaries, skipping transfer
I0307 10:36:57.564524   35916 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0307 10:36:57.570250   35916 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0307 10:36:57.581144   35916 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0307 10:36:57.591900   35916 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)
I0307 10:36:57.603334   35916 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0307 10:36:57.605794   35916 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0307 10:36:57.612912   35916 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0307 10:36:57.659886   35916 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0307 10:36:57.688604   35916 certs.go:68] Setting up /Users/kunaldange/.minikube/profiles/minikube for IP: 192.168.49.2
I0307 10:36:57.688781   35916 certs.go:194] generating shared ca certs ...
I0307 10:36:57.688798   35916 certs.go:226] acquiring lock for ca certs: {Name:mk38ee31c6a0b4d363d91733d5b5a423125a22b0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0307 10:36:57.690231   35916 certs.go:235] skipping valid "minikubeCA" ca cert: /Users/kunaldange/.minikube/ca.key
I0307 10:36:57.690737   35916 certs.go:235] skipping valid "proxyClientCA" ca cert: /Users/kunaldange/.minikube/proxy-client-ca.key
I0307 10:36:57.690760   35916 certs.go:256] generating profile certs ...
I0307 10:36:57.691417   35916 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /Users/kunaldange/.minikube/profiles/minikube/client.key
I0307 10:36:57.692544   35916 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /Users/kunaldange/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0307 10:36:57.693442   35916 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /Users/kunaldange/.minikube/profiles/minikube/proxy-client.key
I0307 10:36:57.694754   35916 certs.go:484] found cert: /Users/kunaldange/.minikube/certs/ca-key.pem (1675 bytes)
I0307 10:36:57.694925   35916 certs.go:484] found cert: /Users/kunaldange/.minikube/certs/ca.pem (1086 bytes)
I0307 10:36:57.695030   35916 certs.go:484] found cert: /Users/kunaldange/.minikube/certs/cert.pem (1131 bytes)
I0307 10:36:57.695126   35916 certs.go:484] found cert: /Users/kunaldange/.minikube/certs/key.pem (1675 bytes)
I0307 10:36:57.704378   35916 ssh_runner.go:362] scp /Users/kunaldange/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0307 10:36:57.720185   35916 ssh_runner.go:362] scp /Users/kunaldange/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0307 10:36:57.735712   35916 ssh_runner.go:362] scp /Users/kunaldange/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0307 10:36:57.751607   35916 ssh_runner.go:362] scp /Users/kunaldange/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0307 10:36:57.767018   35916 ssh_runner.go:362] scp /Users/kunaldange/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0307 10:36:57.782679   35916 ssh_runner.go:362] scp /Users/kunaldange/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0307 10:36:57.797868   35916 ssh_runner.go:362] scp /Users/kunaldange/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0307 10:36:57.813822   35916 ssh_runner.go:362] scp /Users/kunaldange/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0307 10:36:57.829572   35916 ssh_runner.go:362] scp /Users/kunaldange/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0307 10:36:57.846079   35916 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0307 10:36:57.860397   35916 ssh_runner.go:195] Run: openssl version
I0307 10:36:57.869001   35916 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0307 10:36:57.876626   35916 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0307 10:36:57.879289   35916 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Mar  5 14:19 /usr/share/ca-certificates/minikubeCA.pem
I0307 10:36:57.879410   35916 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0307 10:36:57.884583   35916 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0307 10:36:57.891056   35916 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0307 10:36:57.893810   35916 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0307 10:36:57.899213   35916 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0307 10:36:57.904861   35916 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0307 10:36:57.910859   35916 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0307 10:36:57.916977   35916 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0307 10:36:57.924850   35916 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0307 10:36:57.933353   35916 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0307 10:36:57.933566   35916 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0307 10:36:58.000930   35916 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0307 10:36:58.007688   35916 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I0307 10:36:58.007965   35916 kubeadm.go:593] restartPrimaryControlPlane start ...
I0307 10:36:58.008259   35916 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0307 10:36:58.015676   35916 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0307 10:36:58.015800   35916 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0307 10:36:58.053784   35916 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:50511"
I0307 10:36:58.054042   35916 kubeconfig.go:47] verify endpoint returned: got: 127.0.0.1:50511, want: 127.0.0.1:51961
I0307 10:36:58.054287   35916 kubeconfig.go:62] /Users/kunaldange/.kube/config needs updating (will repair): [kubeconfig needs server address update]
I0307 10:36:58.055316   35916 lock.go:35] WriteFile acquiring /Users/kunaldange/.kube/config: {Name:mk754ddd75c9b3901053e50081a41bfa5a2ba2a2 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0307 10:36:58.068878   35916 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0307 10:36:58.076790   35916 kubeadm.go:630] The running cluster does not require reconfiguration: 127.0.0.1
I0307 10:36:58.076805   35916 kubeadm.go:597] duration metric: took 68.837917ms to restartPrimaryControlPlane
I0307 10:36:58.076810   35916 kubeadm.go:394] duration metric: took 143.478ms to StartCluster
I0307 10:36:58.076819   35916 settings.go:142] acquiring lock: {Name:mkcdd021de0d7f4593e98521fc599baa8e8bed20 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0307 10:36:58.076959   35916 settings.go:150] Updating kubeconfig:  /Users/kunaldange/.kube/config
I0307 10:36:58.077334   35916 lock.go:35] WriteFile acquiring /Users/kunaldange/.kube/config: {Name:mk754ddd75c9b3901053e50081a41bfa5a2ba2a2 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0307 10:36:58.078101   35916 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0307 10:36:58.078650   35916 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0307 10:36:58.078299   35916 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0307 10:36:58.078802   35916 addons.go:69] Setting dashboard=true in profile "minikube"
I0307 10:36:58.078803   35916 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0307 10:36:58.078810   35916 addons.go:238] Setting addon dashboard=true in "minikube"
I0307 10:36:58.078805   35916 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0307 10:36:58.078812   35916 addons.go:238] Setting addon storage-provisioner=true in "minikube"
W0307 10:36:58.078813   35916 addons.go:247] addon dashboard should already be in state true
W0307 10:36:58.078815   35916 addons.go:247] addon storage-provisioner should already be in state true
I0307 10:36:58.078981   35916 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0307 10:36:58.079095   35916 host.go:66] Checking if "minikube" exists ...
I0307 10:36:58.079096   35916 host.go:66] Checking if "minikube" exists ...
I0307 10:36:58.079569   35916 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0307 10:36:58.079716   35916 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0307 10:36:58.079720   35916 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0307 10:36:58.086737   35916 out.go:177] 🔎  Verifying Kubernetes components...
I0307 10:36:58.091533   35916 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0307 10:36:58.112345   35916 addons.go:238] Setting addon default-storageclass=true in "minikube"
W0307 10:36:58.112356   35916 addons.go:247] addon default-storageclass should already be in state true
I0307 10:36:58.112374   35916 host.go:66] Checking if "minikube" exists ...
I0307 10:36:58.112830   35916 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0307 10:36:58.116759   35916 out.go:177]     ▪ Using image docker.io/kubernetesui/dashboard:v2.7.0
I0307 10:36:58.121461   35916 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0307 10:36:58.129642   35916 out.go:177]     ▪ Using image docker.io/kubernetesui/metrics-scraper:v1.0.8
I0307 10:36:58.129824   35916 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0307 10:36:58.129830   35916 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0307 10:36:58.129924   35916 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0307 10:36:58.133630   35916 addons.go:435] installing /etc/kubernetes/addons/dashboard-ns.yaml
I0307 10:36:58.133662   35916 ssh_runner.go:362] scp dashboard/dashboard-ns.yaml --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I0307 10:36:58.133817   35916 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0307 10:36:58.146330   35916 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0307 10:36:58.146364   35916 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0307 10:36:58.146596   35916 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0307 10:36:58.162852   35916 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51957 SSHKeyPath:/Users/kunaldange/.minikube/machines/minikube/id_rsa Username:docker}
I0307 10:36:58.165308   35916 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51957 SSHKeyPath:/Users/kunaldange/.minikube/machines/minikube/id_rsa Username:docker}
I0307 10:36:58.167814   35916 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0307 10:36:58.173600   35916 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51957 SSHKeyPath:/Users/kunaldange/.minikube/machines/minikube/id_rsa Username:docker}
I0307 10:36:58.203541   35916 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0307 10:36:58.230733   35916 api_server.go:52] waiting for apiserver process to appear ...
I0307 10:36:58.230861   35916 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0307 10:36:58.282795   35916 addons.go:435] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I0307 10:36:58.282810   35916 ssh_runner.go:362] scp dashboard/dashboard-clusterrole.yaml --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I0307 10:36:58.283543   35916 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0307 10:36:58.286456   35916 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0307 10:36:58.298006   35916 addons.go:435] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I0307 10:36:58.298020   35916 ssh_runner.go:362] scp dashboard/dashboard-clusterrolebinding.yaml --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I0307 10:36:58.312397   35916 addons.go:435] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I0307 10:36:58.312407   35916 ssh_runner.go:362] scp dashboard/dashboard-configmap.yaml --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I0307 10:36:58.325732   35916 addons.go:435] installing /etc/kubernetes/addons/dashboard-dp.yaml
I0307 10:36:58.325739   35916 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I0307 10:36:58.339009   35916 addons.go:435] installing /etc/kubernetes/addons/dashboard-role.yaml
I0307 10:36:58.339020   35916 ssh_runner.go:362] scp dashboard/dashboard-role.yaml --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I0307 10:36:58.354845   35916 addons.go:435] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I0307 10:36:58.354864   35916 ssh_runner.go:362] scp dashboard/dashboard-rolebinding.yaml --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I0307 10:36:58.375170   35916 addons.go:435] installing /etc/kubernetes/addons/dashboard-sa.yaml
I0307 10:36:58.375180   35916 ssh_runner.go:362] scp dashboard/dashboard-sa.yaml --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I0307 10:36:58.404811   35916 addons.go:435] installing /etc/kubernetes/addons/dashboard-secret.yaml
I0307 10:36:58.404820   35916 ssh_runner.go:362] scp dashboard/dashboard-secret.yaml --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I0307 10:36:58.422330   35916 addons.go:435] installing /etc/kubernetes/addons/dashboard-svc.yaml
I0307 10:36:58.422345   35916 ssh_runner.go:362] scp dashboard/dashboard-svc.yaml --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I0307 10:36:58.442519   35916 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
W0307 10:36:58.602235   35916 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0307 10:36:58.602325   35916 retry.go:31] will retry after 296.265769ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0307 10:36:58.602451   35916 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0307 10:36:58.602460   35916 retry.go:31] will retry after 310.51873ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0307 10:36:58.610551   35916 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0307 10:36:58.610578   35916 retry.go:31] will retry after 296.678055ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0307 10:36:58.732036   35916 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0307 10:36:58.899214   35916 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0307 10:36:58.907669   35916 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0307 10:36:58.913200   35916 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0307 10:36:59.027934   35916 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0307 10:36:59.027958   35916 retry.go:31] will retry after 192.272222ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0307 10:36:59.033204   35916 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0307 10:36:59.033228   35916 retry.go:31] will retry after 317.859455ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0307 10:36:59.033260   35916 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0307 10:36:59.033267   35916 retry.go:31] will retry after 555.125471ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0307 10:36:59.220529   35916 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0307 10:36:59.231706   35916 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0307 10:36:59.330610   35916 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0307 10:36:59.330632   35916 api_server.go:72] duration metric: took 1.25255775s to wait for apiserver process to appear ...
I0307 10:36:59.330634   35916 retry.go:31] will retry after 487.587836ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0307 10:36:59.330641   35916 api_server.go:88] waiting for apiserver healthz status ...
I0307 10:36:59.330895   35916 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51961/healthz ...
I0307 10:36:59.334432   35916 api_server.go:269] stopped: https://127.0.0.1:51961/healthz: Get "https://127.0.0.1:51961/healthz": EOF
I0307 10:36:59.352373   35916 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0307 10:36:59.589794   35916 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0307 10:36:59.819164   35916 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0307 10:36:59.831470   35916 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51961/healthz ...
I0307 10:37:01.234204   35916 api_server.go:279] https://127.0.0.1:51961/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0307 10:37:01.234220   35916 api_server.go:103] status: https://127.0.0.1:51961/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0307 10:37:01.234232   35916 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51961/healthz ...
I0307 10:37:01.307490   35916 api_server.go:279] https://127.0.0.1:51961/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0307 10:37:01.307516   35916 api_server.go:103] status: https://127.0.0.1:51961/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0307 10:37:01.331110   35916 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51961/healthz ...
I0307 10:37:01.408119   35916 api_server.go:279] https://127.0.0.1:51961/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0307 10:37:01.408143   35916 api_server.go:103] status: https://127.0.0.1:51961/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0307 10:37:01.831536   35916 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51961/healthz ...
I0307 10:37:01.837858   35916 api_server.go:279] https://127.0.0.1:51961/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0307 10:37:01.837876   35916 api_server.go:103] status: https://127.0.0.1:51961/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0307 10:37:02.331657   35916 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51961/healthz ...
I0307 10:37:02.336789   35916 api_server.go:279] https://127.0.0.1:51961/healthz returned 200:
ok
I0307 10:37:02.349258   35916 api_server.go:141] control plane version: v1.32.0
I0307 10:37:02.349277   35916 api_server.go:131] duration metric: took 3.018733875s to wait for apiserver health ...
I0307 10:37:02.349466   35916 system_pods.go:43] waiting for kube-system pods to appear ...
I0307 10:37:02.368981   35916 system_pods.go:59] 7 kube-system pods found
I0307 10:37:02.369000   35916 system_pods.go:61] "coredns-668d6bf9bc-jzd5x" [95ed817b-d444-486c-8afd-427e5cffb8df] Running
I0307 10:37:02.369003   35916 system_pods.go:61] "etcd-minikube" [2eddd314-64c0-4274-a014-a0211994bae2] Running
I0307 10:37:02.369005   35916 system_pods.go:61] "kube-apiserver-minikube" [60b0ca4a-b08e-485b-8d8c-c805ce0fc7e0] Running
I0307 10:37:02.369008   35916 system_pods.go:61] "kube-controller-manager-minikube" [071bf855-dbff-4395-a723-e022792b9856] Running
I0307 10:37:02.369010   35916 system_pods.go:61] "kube-proxy-82qj8" [6e8905b3-70be-4daf-abee-0421eef08fdd] Running
I0307 10:37:02.369012   35916 system_pods.go:61] "kube-scheduler-minikube" [ae823053-b760-45bc-b8a4-8eb406f62a6c] Running
I0307 10:37:02.369014   35916 system_pods.go:61] "storage-provisioner" [ade4680f-683a-46c9-b34d-79651e56bf73] Running
I0307 10:37:02.369017   35916 system_pods.go:74] duration metric: took 19.547709ms to wait for pod list to return data ...
I0307 10:37:02.369024   35916 kubeadm.go:582] duration metric: took 4.291056292s to wait for: map[apiserver:true system_pods:true]
I0307 10:37:02.369032   35916 node_conditions.go:102] verifying NodePressure condition ...
I0307 10:37:02.375202   35916 node_conditions.go:122] node storage ephemeral capacity is 1055761844Ki
I0307 10:37:02.375219   35916 node_conditions.go:123] node cpu capacity is 8
I0307 10:37:02.375248   35916 node_conditions.go:105] duration metric: took 6.212208ms to run NodePressure ...
I0307 10:37:02.375260   35916 start.go:241] waiting for startup goroutines ...
I0307 10:37:02.659566   35916 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (3.307272959s)
I0307 10:37:02.659602   35916 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (3.069868584s)
I0307 10:37:02.659698   35916 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (2.84061275s)
I0307 10:37:02.664607   35916 out.go:177] 💡  Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server

I0307 10:37:02.680733   35916 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass, dashboard
I0307 10:37:02.684644   35916 addons.go:514] duration metric: took 4.607038792s for enable addons: enabled=[storage-provisioner default-storageclass dashboard]
I0307 10:37:02.684691   35916 start.go:246] waiting for cluster config update ...
I0307 10:37:02.684723   35916 start.go:255] writing updated cluster config ...
I0307 10:37:02.690360   35916 ssh_runner.go:195] Run: rm -f paused
I0307 10:37:02.764827   35916 start.go:600] kubectl: 1.31.4, cluster: 1.32.0 (minor skew: 1)
I0307 10:37:02.769653   35916 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Mar 07 07:04:23 minikube cri-dockerd[1312]: time="2025-03-07T07:04:23Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Mar 07 07:04:28 minikube cri-dockerd[1312]: time="2025-03-07T07:04:28Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Mar 07 07:04:30 minikube cri-dockerd[1312]: time="2025-03-07T07:04:30Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Mar 07 07:04:37 minikube cri-dockerd[1312]: time="2025-03-07T07:04:37Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Mar 07 07:04:39 minikube cri-dockerd[1312]: time="2025-03-07T07:04:39Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Mar 07 07:04:43 minikube cri-dockerd[1312]: time="2025-03-07T07:04:43Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Mar 07 07:04:44 minikube dockerd[999]: time="2025-03-07T07:04:44.080046635Z" level=info msg="ignoring event" container=fd9364f3b8c22ba9f9e3611d186e8e48de19bb488b8ec645b4e065754e6ee90a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 07 07:04:44 minikube dockerd[999]: time="2025-03-07T07:04:44.390992385Z" level=info msg="ignoring event" container=7719a4312dc694fb3bab9821233c795b83cc25056a18049d92dc36ee71699a50 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 07 07:04:44 minikube dockerd[999]: time="2025-03-07T07:04:44.433590426Z" level=info msg="ignoring event" container=540d74baf86ee5a1198c6cb3db6809ec45ca53679dedfa20b59c295619140a2a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 07 07:04:44 minikube dockerd[999]: time="2025-03-07T07:04:44.486075593Z" level=info msg="ignoring event" container=5abfc213539b18c6e0a0b2d052bd777549def2810779169c79edd218d4d761c4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 07 07:04:44 minikube dockerd[999]: time="2025-03-07T07:04:44.508940093Z" level=info msg="ignoring event" container=58a7c661547a2cc0bf85bd1e81acf252df9d39733a27d51d72326c9b0e4b243d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 07 07:04:44 minikube dockerd[999]: time="2025-03-07T07:04:44.533044843Z" level=info msg="ignoring event" container=8948cbff1ff7687d32ea25d98533aee5820f3e8b46ef02fdc6f7f7e4331a98c9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 07 07:04:49 minikube cri-dockerd[1312]: time="2025-03-07T07:04:49Z" level=error msg="error getting RW layer size for container ID 'fd9364f3b8c22ba9f9e3611d186e8e48de19bb488b8ec645b4e065754e6ee90a': Error response from daemon: No such container: fd9364f3b8c22ba9f9e3611d186e8e48de19bb488b8ec645b4e065754e6ee90a"
Mar 07 07:04:49 minikube cri-dockerd[1312]: time="2025-03-07T07:04:49Z" level=error msg="Set backoffDuration to : 1m0s for container ID 'fd9364f3b8c22ba9f9e3611d186e8e48de19bb488b8ec645b4e065754e6ee90a'"
Mar 07 07:04:49 minikube cri-dockerd[1312]: time="2025-03-07T07:04:49Z" level=error msg="error getting RW layer size for container ID 'a8d5b729713bcde6a2a5cb4563c776c7ffc223b41cc70ddf1959aea96c4989d2': Error response from daemon: No such container: a8d5b729713bcde6a2a5cb4563c776c7ffc223b41cc70ddf1959aea96c4989d2"
Mar 07 07:04:49 minikube cri-dockerd[1312]: time="2025-03-07T07:04:49Z" level=error msg="Set backoffDuration to : 1m0s for container ID 'a8d5b729713bcde6a2a5cb4563c776c7ffc223b41cc70ddf1959aea96c4989d2'"
Mar 07 07:06:49 minikube cri-dockerd[1312]: time="2025-03-07T07:06:49Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2d51fea1e6d12ffa06bc92c16b561fad1854b7452912177bae462db49e4df7c9/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 07 07:06:49 minikube cri-dockerd[1312]: time="2025-03-07T07:06:49Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/36730d2b4413e91c667bc1979134dfe7cd34263052c6ba49ccaf14642d63f784/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 07 07:06:52 minikube cri-dockerd[1312]: time="2025-03-07T07:06:52Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Mar 07 07:06:54 minikube cri-dockerd[1312]: time="2025-03-07T07:06:54Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Mar 07 07:07:00 minikube cri-dockerd[1312]: time="2025-03-07T07:07:00Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/fd978f2ae4e79c4d43b7890828b9a022e9e73e79759cb6840482a54b850cf238/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 07 07:07:00 minikube cri-dockerd[1312]: time="2025-03-07T07:07:00Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3b5e8718c80023ea8424823b8fd3a7887241b0cb6da3397932e6461e145649b1/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 07 07:07:03 minikube cri-dockerd[1312]: time="2025-03-07T07:07:03Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Mar 07 07:07:05 minikube cri-dockerd[1312]: time="2025-03-07T07:07:05Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Mar 07 07:07:07 minikube cri-dockerd[1312]: time="2025-03-07T07:07:07Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Mar 07 07:07:10 minikube cri-dockerd[1312]: time="2025-03-07T07:07:10Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Mar 07 07:07:24 minikube cri-dockerd[1312]: time="2025-03-07T07:07:24Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Mar 07 07:07:26 minikube cri-dockerd[1312]: time="2025-03-07T07:07:26Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Mar 07 07:07:37 minikube cri-dockerd[1312]: time="2025-03-07T07:07:37Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Mar 07 07:07:42 minikube cri-dockerd[1312]: time="2025-03-07T07:07:42Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Mar 07 07:07:52 minikube cri-dockerd[1312]: time="2025-03-07T07:07:52Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Mar 07 07:07:55 minikube cri-dockerd[1312]: time="2025-03-07T07:07:55Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Mar 07 07:08:06 minikube cri-dockerd[1312]: time="2025-03-07T07:08:06Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Mar 07 07:08:11 minikube cri-dockerd[1312]: time="2025-03-07T07:08:11Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Mar 07 07:08:22 minikube cri-dockerd[1312]: time="2025-03-07T07:08:22Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Mar 07 07:08:29 minikube cri-dockerd[1312]: time="2025-03-07T07:08:29Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Mar 07 07:08:40 minikube cri-dockerd[1312]: time="2025-03-07T07:08:40Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Mar 07 07:08:46 minikube cri-dockerd[1312]: time="2025-03-07T07:08:46Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Mar 07 07:08:57 minikube cri-dockerd[1312]: time="2025-03-07T07:08:57Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Mar 07 07:09:03 minikube cri-dockerd[1312]: time="2025-03-07T07:09:03Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Mar 07 07:09:10 minikube cri-dockerd[1312]: time="2025-03-07T07:09:10Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Mar 07 07:09:17 minikube cri-dockerd[1312]: time="2025-03-07T07:09:17Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Mar 07 07:09:23 minikube cri-dockerd[1312]: time="2025-03-07T07:09:23Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Mar 07 07:09:33 minikube cri-dockerd[1312]: time="2025-03-07T07:09:33Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Mar 07 07:09:37 minikube cri-dockerd[1312]: time="2025-03-07T07:09:37Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Mar 07 07:09:49 minikube cri-dockerd[1312]: time="2025-03-07T07:09:49Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Mar 07 07:09:54 minikube cri-dockerd[1312]: time="2025-03-07T07:09:54Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Mar 07 07:10:06 minikube cri-dockerd[1312]: time="2025-03-07T07:10:06Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Mar 07 07:10:11 minikube cri-dockerd[1312]: time="2025-03-07T07:10:11Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Mar 07 07:10:22 minikube cri-dockerd[1312]: time="2025-03-07T07:10:22Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Mar 07 07:10:27 minikube cri-dockerd[1312]: time="2025-03-07T07:10:27Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Mar 07 07:10:37 minikube cri-dockerd[1312]: time="2025-03-07T07:10:37Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Mar 07 07:10:46 minikube cri-dockerd[1312]: time="2025-03-07T07:10:46Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Mar 07 07:10:50 minikube cri-dockerd[1312]: time="2025-03-07T07:10:50Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Mar 07 07:11:00 minikube cri-dockerd[1312]: time="2025-03-07T07:11:00Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Mar 07 07:11:06 minikube cri-dockerd[1312]: time="2025-03-07T07:11:06Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Mar 07 07:11:16 minikube cri-dockerd[1312]: time="2025-03-07T07:11:16Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Mar 07 07:11:24 minikube cri-dockerd[1312]: time="2025-03-07T07:11:24Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Mar 07 07:11:32 minikube cri-dockerd[1312]: time="2025-03-07T07:11:32Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Mar 07 07:11:39 minikube cri-dockerd[1312]: time="2025-03-07T07:11:39Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"


==> container status <==
CONTAINER           IMAGE                                                                           CREATED             STATE               NAME                        ATTEMPT             POD ID              POD
d587c136b9612       mongo@sha256:f6164e498dbaee3966031c1ced1bfa0fd1c4961151b38a770fa6994f4b0dcae7   4 minutes ago       Running             mongo                       0                   2d51fea1e6d12       mongo-deployment-74ff5b95db-wp457
3d9f67b457d0d       mongo@sha256:f6164e498dbaee3966031c1ced1bfa0fd1c4961151b38a770fa6994f4b0dcae7   4 minutes ago       Running             mongo                       0                   36730d2b4413e       mongo-deployment-74ff5b95db-7jssg
0891f1f98c6ee       20b332c9a70d8                                                                   2 hours ago         Running             kubernetes-dashboard        4                   77441abf70a49       kubernetes-dashboard-7779f9b69b-vbrr6
182d2a35e1f5b       ba04bb24b9575                                                                   2 hours ago         Running             storage-provisioner         5                   226f89a00b377       storage-provisioner
c08a2dda004f0       a422e0e982356                                                                   2 hours ago         Running             dashboard-metrics-scraper   2                   94dde919b03fa       dashboard-metrics-scraper-5d59dccf9b-zc9l9
7e6e0d13db3b0       20b332c9a70d8                                                                   2 hours ago         Exited              kubernetes-dashboard        3                   77441abf70a49       kubernetes-dashboard-7779f9b69b-vbrr6
c4021d93e6d8b       2f6c962e7b831                                                                   2 hours ago         Running             coredns                     2                   4b07d0a9ce70c       coredns-668d6bf9bc-jzd5x
a6a54303feedc       ba04bb24b9575                                                                   2 hours ago         Exited              storage-provisioner         4                   226f89a00b377       storage-provisioner
cef1842c2fa57       2f50386e20bfd                                                                   2 hours ago         Running             kube-proxy                  2                   ac15cb263df82       kube-proxy-82qj8
3354c2661744b       7fc9d4aa817aa                                                                   2 hours ago         Running             etcd                        2                   0dec4fd4490bb       etcd-minikube
ce6075c0d2b67       2b5bd0f16085a                                                                   2 hours ago         Running             kube-apiserver              2                   a769c1c5f4fc9       kube-apiserver-minikube
1767af9b375bc       a8d049396f6b8                                                                   2 hours ago         Running             kube-controller-manager     2                   1dabd576c961e       kube-controller-manager-minikube
68fb2d794eb84       c3ff26fb59f37                                                                   2 hours ago         Running             kube-scheduler              2                   e195bf0d8a729       kube-scheduler-minikube
7e5897c1d01f9       2f6c962e7b831                                                                   15 hours ago        Exited              coredns                     1                   e8e7057fb82fb       coredns-668d6bf9bc-jzd5x
03f6dd9eaa6c1       a422e0e982356                                                                   15 hours ago        Exited              dashboard-metrics-scraper   1                   73e6c212e1647       dashboard-metrics-scraper-5d59dccf9b-zc9l9
67039f9efb575       2f50386e20bfd                                                                   15 hours ago        Exited              kube-proxy                  1                   2a98e8aa43aa1       kube-proxy-82qj8
567d0dabfabcc       7fc9d4aa817aa                                                                   15 hours ago        Exited              etcd                        1                   5bec054552eaf       etcd-minikube
afcdf99b28449       a8d049396f6b8                                                                   15 hours ago        Exited              kube-controller-manager     1                   fad593db69bc9       kube-controller-manager-minikube
9f38e751b456d       c3ff26fb59f37                                                                   15 hours ago        Exited              kube-scheduler              1                   2c27ddca06d4b       kube-scheduler-minikube
a37da1271f25f       2b5bd0f16085a                                                                   15 hours ago        Exited              kube-apiserver              1                   17c74432d16d0       kube-apiserver-minikube


==> coredns [7e5897c1d01f] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.11.3
linux/arm64, go1.21.11, a6338e9
[INFO] 127.0.0.1:60026 - 45780 "HINFO IN 1013462069439934131.4106664217637801099. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.038535917s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[703464332]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (06-Mar-2025 16:01:38.857) (total time: 30003ms):
Trace[703464332]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30003ms (16:02:08.860)
Trace[703464332]: [30.003465305s] [30.003465305s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[975910476]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (06-Mar-2025 16:01:38.857) (total time: 30003ms):
Trace[975910476]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30003ms (16:02:08.860)
Trace[975910476]: [30.003625638s] [30.003625638s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[495491360]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (06-Mar-2025 16:01:38.857) (total time: 30003ms):
Trace[495491360]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30003ms (16:02:08.860)
Trace[495491360]: [30.003681055s] [30.003681055s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> coredns [c4021d93e6d8] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.11.3
linux/arm64, go1.21.11, a6338e9
[INFO] 127.0.0.1:45314 - 54198 "HINFO IN 6202971335246610069.7023812737487844119. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.038595791s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[326737122]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (07-Mar-2025 05:07:04.999) (total time: 30005ms):
Trace[326737122]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30003ms (05:07:35.001)
Trace[326737122]: [30.005319847s] [30.005319847s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[636547062]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (07-Mar-2025 05:07:04.999) (total time: 30004ms):
Trace[636547062]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30003ms (05:07:35.001)
Trace[636547062]: [30.00459718s] [30.00459718s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[269313720]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (07-Mar-2025 05:07:05.000) (total time: 30005ms):
Trace[269313720]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30003ms (05:07:35.001)
Trace[269313720]: [30.005484305s] [30.005484305s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] 10.244.0.26:35520 - 4326 "AAAA IN compass.mongodb.com.default.svc.cluster.local. udp 63 false 512" NXDOMAIN qr,aa,rd 156 0.010962667s
[INFO] 10.244.0.26:35520 - 55011 "A IN compass.mongodb.com.default.svc.cluster.local. udp 63 false 512" NXDOMAIN qr,aa,rd 156 0.011920625s
[INFO] 10.244.0.26:58587 - 38440 "AAAA IN compass.mongodb.com.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.000162542s
[INFO] 10.244.0.26:58587 - 292 "A IN compass.mongodb.com.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.000098417s
[INFO] 10.244.0.26:57635 - 56430 "AAAA IN compass.mongodb.com.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000056458s
[INFO] 10.244.0.26:57635 - 27243 "A IN compass.mongodb.com.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000041125s
[INFO] 10.244.0.26:44554 - 1772 "A IN compass.mongodb.com. udp 37 false 512" NOERROR qr,rd,ra 306 0.024529208s
[INFO] 10.244.0.26:44554 - 53999 "AAAA IN compass.mongodb.com. udp 37 false 512" NOERROR qr,rd,ra 96 0.038289583s
[INFO] 10.244.0.27:42807 - 52691 "AAAA IN compass.mongodb.com.default.svc.cluster.local. udp 63 false 512" NXDOMAIN qr,aa,rd 156 0.000393125s
[INFO] 10.244.0.27:42807 - 28847 "A IN compass.mongodb.com.default.svc.cluster.local. udp 63 false 512" NXDOMAIN qr,aa,rd 156 0.000573667s
[INFO] 10.244.0.27:37611 - 21429 "AAAA IN compass.mongodb.com.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.000113167s
[INFO] 10.244.0.27:37611 - 58811 "A IN compass.mongodb.com.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.000194792s
[INFO] 10.244.0.27:37572 - 49714 "AAAA IN compass.mongodb.com.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000081083s
[INFO] 10.244.0.27:37572 - 22320 "A IN compass.mongodb.com.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000105791s
[INFO] 10.244.0.27:60925 - 43186 "AAAA IN compass.mongodb.com. udp 37 false 512" NOERROR qr,aa,rd,ra 96 0.00009675s
[INFO] 10.244.0.27:60925 - 44979 "A IN compass.mongodb.com. udp 37 false 512" NOERROR qr,aa,rd,ra 306 0.000076541s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=arm64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=arm64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_03_05T19_49_11_0700
                    minikube.k8s.io/version=v1.35.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 05 Mar 2025 14:19:09 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Fri, 07 Mar 2025 07:11:41 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 07 Mar 2025 07:09:42 +0000   Fri, 07 Mar 2025 05:07:01 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 07 Mar 2025 07:09:42 +0000   Fri, 07 Mar 2025 05:07:01 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 07 Mar 2025 07:09:42 +0000   Fri, 07 Mar 2025 05:07:01 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 07 Mar 2025 07:09:42 +0000   Fri, 07 Mar 2025 05:07:01 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  1055761844Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             4013032Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  1055761844Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             4013032Ki
  pods:               110
System Info:
  Machine ID:                 3f2bb1119e9a4b87ac8a3bbbfba4793d
  System UUID:                3f2bb1119e9a4b87ac8a3bbbfba4793d
  Boot ID:                    5b7f9f60-db73-4e73-bc99-49456e2b600f
  Kernel Version:             6.12.5-linuxkit
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               arm64
  Container Runtime Version:  docker://27.4.1
  Kubelet Version:            v1.32.0
  Kube-Proxy Version:         v1.32.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (13 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  default                     mongo-deployment-74ff5b95db-7jssg             0 (0%)        0 (0%)      0 (0%)           0 (0%)         4m52s
  default                     mongo-deployment-74ff5b95db-wp457             0 (0%)        0 (0%)      0 (0%)           0 (0%)         4m52s
  default                     webapp-deployment-74f4657677-62p9k            0 (0%)        0 (0%)      0 (0%)           0 (0%)         4m42s
  default                     webapp-deployment-74f4657677-q2jmp            0 (0%)        0 (0%)      0 (0%)           0 (0%)         4m42s
  kube-system                 coredns-668d6bf9bc-jzd5x                      100m (1%)     0 (0%)      70Mi (1%)        170Mi (4%)     40h
  kube-system                 etcd-minikube                                 100m (1%)     0 (0%)      100Mi (2%)       0 (0%)         40h
  kube-system                 kube-apiserver-minikube                       250m (3%)     0 (0%)      0 (0%)           0 (0%)         40h
  kube-system                 kube-controller-manager-minikube              200m (2%)     0 (0%)      0 (0%)           0 (0%)         40h
  kube-system                 kube-proxy-82qj8                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         40h
  kube-system                 kube-scheduler-minikube                       100m (1%)     0 (0%)      0 (0%)           0 (0%)         40h
  kube-system                 storage-provisioner                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         40h
  kubernetes-dashboard        dashboard-metrics-scraper-5d59dccf9b-zc9l9    0 (0%)        0 (0%)      0 (0%)           0 (0%)         40h
  kubernetes-dashboard        kubernetes-dashboard-7779f9b69b-vbrr6         0 (0%)        0 (0%)      0 (0%)           0 (0%)         40h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (9%)   0 (0%)
  memory             170Mi (4%)  170Mi (4%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
  hugepages-32Mi     0 (0%)      0 (0%)
  hugepages-64Ki     0 (0%)      0 (0%)
Events:              <none>


==> dmesg <==


==> etcd [3354c2661744] <==
{"level":"info","ts":"2025-03-07T05:37:00.320714Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4604}
{"level":"info","ts":"2025-03-07T05:37:00.326057Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":4604,"took":"4.415792ms","hash":1521802412,"current-db-size-bytes":3330048,"current-db-size":"3.3 MB","current-db-size-in-use-bytes":1642496,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2025-03-07T05:37:00.326164Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1521802412,"revision":4604,"compact-revision":4364}
{"level":"info","ts":"2025-03-07T05:42:00.314976Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4846}
{"level":"info","ts":"2025-03-07T05:42:00.320227Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":4846,"took":"4.066792ms","hash":281801154,"current-db-size-bytes":3330048,"current-db-size":"3.3 MB","current-db-size-in-use-bytes":1605632,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2025-03-07T05:42:00.320313Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":281801154,"revision":4846,"compact-revision":4604}
{"level":"info","ts":"2025-03-07T05:47:00.306818Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5085}
{"level":"info","ts":"2025-03-07T05:47:00.311864Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":5085,"took":"4.311833ms","hash":4172687664,"current-db-size-bytes":3330048,"current-db-size":"3.3 MB","current-db-size-in-use-bytes":1597440,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2025-03-07T05:47:00.311929Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":4172687664,"revision":5085,"compact-revision":4846}
{"level":"info","ts":"2025-03-07T05:52:00.449761Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5325}
{"level":"info","ts":"2025-03-07T05:52:00.454504Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":5325,"took":"3.771375ms","hash":703247930,"current-db-size-bytes":3330048,"current-db-size":"3.3 MB","current-db-size-in-use-bytes":1613824,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2025-03-07T05:52:00.454597Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":703247930,"revision":5325,"compact-revision":5085}
{"level":"info","ts":"2025-03-07T05:57:00.454381Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5565}
{"level":"info","ts":"2025-03-07T05:57:00.491839Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":5565,"took":"34.173833ms","hash":4242746865,"current-db-size-bytes":3551232,"current-db-size":"3.6 MB","current-db-size-in-use-bytes":1961984,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2025-03-07T05:57:00.492434Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":4242746865,"revision":5565,"compact-revision":5325}
{"level":"info","ts":"2025-03-07T06:02:00.453398Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5841}
{"level":"info","ts":"2025-03-07T06:02:00.468840Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":5841,"took":"14.631292ms","hash":738957639,"current-db-size-bytes":3887104,"current-db-size":"3.9 MB","current-db-size-in-use-bytes":2555904,"current-db-size-in-use":"2.6 MB"}
{"level":"info","ts":"2025-03-07T06:02:00.468947Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":738957639,"revision":5841,"compact-revision":5565}
{"level":"info","ts":"2025-03-07T06:07:00.457949Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6092}
{"level":"info","ts":"2025-03-07T06:07:00.479266Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":6092,"took":"19.447209ms","hash":1360979460,"current-db-size-bytes":3887104,"current-db-size":"3.9 MB","current-db-size-in-use-bytes":2355200,"current-db-size-in-use":"2.4 MB"}
{"level":"info","ts":"2025-03-07T06:07:00.479367Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1360979460,"revision":6092,"compact-revision":5841}
{"level":"info","ts":"2025-03-07T06:12:00.465019Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6380}
{"level":"info","ts":"2025-03-07T06:12:00.492609Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":6380,"took":"25.470125ms","hash":413035279,"current-db-size-bytes":3887104,"current-db-size":"3.9 MB","current-db-size-in-use-bytes":2269184,"current-db-size-in-use":"2.3 MB"}
{"level":"info","ts":"2025-03-07T06:12:00.492746Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":413035279,"revision":6380,"compact-revision":6092}
{"level":"info","ts":"2025-03-07T06:17:00.467049Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6702}
{"level":"info","ts":"2025-03-07T06:17:00.490249Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":6702,"took":"21.602875ms","hash":522504766,"current-db-size-bytes":3887104,"current-db-size":"3.9 MB","current-db-size-in-use-bytes":2617344,"current-db-size-in-use":"2.6 MB"}
{"level":"info","ts":"2025-03-07T06:17:00.490318Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":522504766,"revision":6702,"compact-revision":6380}
{"level":"info","ts":"2025-03-07T06:22:00.467537Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":6984}
{"level":"info","ts":"2025-03-07T06:22:00.504556Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":6984,"took":"35.689333ms","hash":2920614786,"current-db-size-bytes":4120576,"current-db-size":"4.1 MB","current-db-size-in-use-bytes":2928640,"current-db-size-in-use":"2.9 MB"}
{"level":"info","ts":"2025-03-07T06:22:00.504625Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2920614786,"revision":6984,"compact-revision":6702}
{"level":"info","ts":"2025-03-07T06:27:00.470299Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7386}
{"level":"info","ts":"2025-03-07T06:27:00.485298Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":7386,"took":"13.199625ms","hash":3443238116,"current-db-size-bytes":4120576,"current-db-size":"4.1 MB","current-db-size-in-use-bytes":2670592,"current-db-size-in-use":"2.7 MB"}
{"level":"info","ts":"2025-03-07T06:27:00.485407Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3443238116,"revision":7386,"compact-revision":6984}
{"level":"info","ts":"2025-03-07T06:34:41.197761Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7691}
{"level":"info","ts":"2025-03-07T06:34:41.214532Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":7691,"took":"14.881541ms","hash":2336220368,"current-db-size-bytes":4120576,"current-db-size":"4.1 MB","current-db-size-in-use-bytes":2203648,"current-db-size-in-use":"2.2 MB"}
{"level":"info","ts":"2025-03-07T06:34:41.214611Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2336220368,"revision":7691,"compact-revision":7386}
{"level":"info","ts":"2025-03-07T06:39:03.634213Z","caller":"etcdserver/server.go:1473","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":10001,"local-member-snapshot-index":0,"local-member-snapshot-count":10000}
{"level":"info","ts":"2025-03-07T06:39:03.642925Z","caller":"etcdserver/server.go:2493","msg":"saved snapshot","snapshot-index":10001}
{"level":"info","ts":"2025-03-07T06:39:03.643241Z","caller":"etcdserver/server.go:2523","msg":"compacted Raft logs","compact-index":5001}
{"level":"info","ts":"2025-03-07T06:39:41.198183Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":7955}
{"level":"info","ts":"2025-03-07T06:39:41.206141Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":7955,"took":"6.900833ms","hash":1287743817,"current-db-size-bytes":4120576,"current-db-size":"4.1 MB","current-db-size-in-use-bytes":1957888,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2025-03-07T06:39:41.206251Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1287743817,"revision":7955,"compact-revision":7691}
{"level":"info","ts":"2025-03-07T06:44:41.204080Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8194}
{"level":"info","ts":"2025-03-07T06:44:41.211296Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":8194,"took":"6.003417ms","hash":3296101147,"current-db-size-bytes":4120576,"current-db-size":"4.1 MB","current-db-size-in-use-bytes":1769472,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-03-07T06:44:41.211368Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3296101147,"revision":8194,"compact-revision":7955}
{"level":"info","ts":"2025-03-07T06:49:41.207598Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8435}
{"level":"info","ts":"2025-03-07T06:49:41.214075Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":8435,"took":"5.7565ms","hash":2046970297,"current-db-size-bytes":4120576,"current-db-size":"4.1 MB","current-db-size-in-use-bytes":1744896,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-03-07T06:49:41.214180Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2046970297,"revision":8435,"compact-revision":8194}
{"level":"info","ts":"2025-03-07T06:54:41.305546Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8674}
{"level":"info","ts":"2025-03-07T06:54:41.310114Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":8674,"took":"3.493708ms","hash":3643493550,"current-db-size-bytes":4120576,"current-db-size":"4.1 MB","current-db-size-in-use-bytes":2093056,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2025-03-07T06:54:41.310188Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3643493550,"revision":8674,"compact-revision":8435}
{"level":"info","ts":"2025-03-07T06:59:41.302895Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":9024}
{"level":"info","ts":"2025-03-07T06:59:41.310286Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":9024,"took":"6.151791ms","hash":882285448,"current-db-size-bytes":4231168,"current-db-size":"4.2 MB","current-db-size-in-use-bytes":2367488,"current-db-size-in-use":"2.4 MB"}
{"level":"info","ts":"2025-03-07T06:59:41.310352Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":882285448,"revision":9024,"compact-revision":8674}
{"level":"info","ts":"2025-03-07T07:04:41.311141Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":9409}
{"level":"info","ts":"2025-03-07T07:04:41.319755Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":9409,"took":"6.973917ms","hash":463271831,"current-db-size-bytes":4231168,"current-db-size":"4.2 MB","current-db-size-in-use-bytes":2039808,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2025-03-07T07:04:41.319911Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":463271831,"revision":9409,"compact-revision":9024}
{"level":"info","ts":"2025-03-07T07:09:41.315660Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":9668}
{"level":"info","ts":"2025-03-07T07:09:41.351394Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":9668,"took":"34.215291ms","hash":52232007,"current-db-size-bytes":4231168,"current-db-size":"4.2 MB","current-db-size-in-use-bytes":3035136,"current-db-size-in-use":"3.0 MB"}
{"level":"info","ts":"2025-03-07T07:09:41.351487Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":52232007,"revision":9668,"compact-revision":9409}


==> etcd [567d0dabfabc] <==
{"level":"info","ts":"2025-03-06T16:01:33.194151Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-03-06T16:01:33.194159Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-03-06T16:01:33.195850Z","caller":"embed/etcd.go:729","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-03-06T16:01:33.195937Z","caller":"embed/etcd.go:600","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-03-06T16:01:33.196012Z","caller":"embed/etcd.go:572","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-03-06T16:01:33.196097Z","caller":"embed/etcd.go:280","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-03-06T16:01:33.196135Z","caller":"embed/etcd.go:871","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-03-06T16:01:34.186404Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 2"}
{"level":"info","ts":"2025-03-06T16:01:34.186458Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 2"}
{"level":"info","ts":"2025-03-06T16:01:34.186493Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-03-06T16:01:34.186513Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 3"}
{"level":"info","ts":"2025-03-06T16:01:34.186518Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 3"}
{"level":"info","ts":"2025-03-06T16:01:34.186531Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 3"}
{"level":"info","ts":"2025-03-06T16:01:34.186537Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 3"}
{"level":"info","ts":"2025-03-06T16:01:34.188114Z","caller":"etcdserver/server.go:2140","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-03-06T16:01:34.188515Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-03-06T16:01:34.188826Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-03-06T16:01:34.188954Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-03-06T16:01:34.189012Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-03-06T16:01:34.192202Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-03-06T16:01:34.192239Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-03-06T16:01:34.194482Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-03-06T16:01:34.195881Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-03-06T16:11:34.299815Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1181}
{"level":"info","ts":"2025-03-06T16:11:34.313690Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":1181,"took":"13.126625ms","hash":4005145167,"current-db-size-bytes":3330048,"current-db-size":"3.3 MB","current-db-size-in-use-bytes":1552384,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2025-03-06T16:11:34.313744Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":4005145167,"revision":1181,"compact-revision":-1}
{"level":"info","ts":"2025-03-06T16:16:34.298896Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1420}
{"level":"info","ts":"2025-03-06T16:16:34.303589Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":1420,"took":"4.079416ms","hash":3562061989,"current-db-size-bytes":3330048,"current-db-size":"3.3 MB","current-db-size-in-use-bytes":1695744,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-03-06T16:16:34.303650Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3562061989,"revision":1420,"compact-revision":1181}
{"level":"info","ts":"2025-03-06T16:21:34.300504Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1661}
{"level":"info","ts":"2025-03-06T16:21:34.304970Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":1661,"took":"3.580959ms","hash":2398303918,"current-db-size-bytes":3330048,"current-db-size":"3.3 MB","current-db-size-in-use-bytes":1654784,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-03-06T16:21:34.305021Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2398303918,"revision":1661,"compact-revision":1420}
{"level":"info","ts":"2025-03-06T16:26:34.305092Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1900}
{"level":"info","ts":"2025-03-06T16:26:34.308893Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":1900,"took":"3.315292ms","hash":1431732728,"current-db-size-bytes":3330048,"current-db-size":"3.3 MB","current-db-size-in-use-bytes":1658880,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-03-06T16:26:34.308959Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1431732728,"revision":1900,"compact-revision":1661}
{"level":"info","ts":"2025-03-06T16:31:34.307664Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2138}
{"level":"info","ts":"2025-03-06T16:31:34.311819Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":2138,"took":"3.675958ms","hash":283550642,"current-db-size-bytes":3330048,"current-db-size":"3.3 MB","current-db-size-in-use-bytes":1634304,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2025-03-06T16:31:34.311878Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":283550642,"revision":2138,"compact-revision":1900}
{"level":"info","ts":"2025-03-06T16:36:34.307936Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2379}
{"level":"info","ts":"2025-03-06T16:36:34.312550Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":2379,"took":"3.919625ms","hash":1466986749,"current-db-size-bytes":3330048,"current-db-size":"3.3 MB","current-db-size-in-use-bytes":1634304,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2025-03-06T16:36:34.312612Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1466986749,"revision":2379,"compact-revision":2138}
{"level":"info","ts":"2025-03-06T16:41:34.310476Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2618}
{"level":"info","ts":"2025-03-06T16:41:34.315016Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":2618,"took":"3.720583ms","hash":530275263,"current-db-size-bytes":3330048,"current-db-size":"3.3 MB","current-db-size-in-use-bytes":1662976,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-03-06T16:41:34.315073Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":530275263,"revision":2618,"compact-revision":2379}
{"level":"info","ts":"2025-03-06T16:46:34.309715Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2859}
{"level":"info","ts":"2025-03-06T16:46:34.312626Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":2859,"took":"2.487292ms","hash":3689063926,"current-db-size-bytes":3330048,"current-db-size":"3.3 MB","current-db-size-in-use-bytes":1630208,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2025-03-06T16:46:34.312674Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3689063926,"revision":2859,"compact-revision":2618}
{"level":"info","ts":"2025-03-06T18:04:57.187852Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3098}
{"level":"info","ts":"2025-03-06T18:04:57.202460Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":3098,"took":"14.210208ms","hash":3427356162,"current-db-size-bytes":3330048,"current-db-size":"3.3 MB","current-db-size-in-use-bytes":1474560,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2025-03-06T18:04:57.202646Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3427356162,"revision":3098,"compact-revision":2859}
{"level":"info","ts":"2025-03-06T18:04:57.999892Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2025-03-06T18:04:58.008545Z","caller":"embed/etcd.go:378","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2025-03-06T18:04:58.085933Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-03-06T18:04:58.086831Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-03-06T18:04:58.281254Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2025-03-06T18:04:58.281393Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2025-03-06T18:04:58.282137Z","caller":"etcdserver/server.go:1543","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2025-03-06T18:04:58.294438Z","caller":"embed/etcd.go:582","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-03-06T18:04:58.295670Z","caller":"embed/etcd.go:587","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-03-06T18:04:58.295757Z","caller":"embed/etcd.go:380","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> kernel <==
 07:11:41 up  2:43,  0 users,  load average: 3.91, 3.40, 3.10
Linux minikube 6.12.5-linuxkit #1 SMP Tue Jan 21 10:23:32 UTC 2025 aarch64 aarch64 aarch64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [a37da1271f25] <==
I0306 18:04:58.281367       1 controller.go:176] quota evaluator worker shutdown
I0306 18:04:58.281373       1 controller.go:176] quota evaluator worker shutdown
I0306 18:04:58.281557       1 crdregistration_controller.go:145] Shutting down crd-autoregister controller
I0306 18:04:58.282402       1 autoregister_controller.go:168] Shutting down autoregister controller
I0306 18:04:58.287882       1 controller.go:132] Ending legacy_token_tracking_controller
I0306 18:04:58.288015       1 controller.go:133] Shutting down legacy_token_tracking_controller
W0306 18:04:59.106822       1 logging.go:55] [core] [Channel #91 SubChannel #92]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.106866       1 logging.go:55] [core] [Channel #70 SubChannel #71]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.106819       1 logging.go:55] [core] [Channel #169 SubChannel #170]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.106894       1 logging.go:55] [core] [Channel #136 SubChannel #137]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.106905       1 logging.go:55] [core] [Channel #52 SubChannel #53]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.106895       1 logging.go:55] [core] [Channel #82 SubChannel #83]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.184470       1 logging.go:55] [core] [Channel #61 SubChannel #62]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.184540       1 logging.go:55] [core] [Channel #139 SubChannel #140]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.184549       1 logging.go:55] [core] [Channel #55 SubChannel #56]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.184470       1 logging.go:55] [core] [Channel #115 SubChannel #116]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.184470       1 logging.go:55] [core] [Channel #175 SubChannel #176]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.184626       1 logging.go:55] [core] [Channel #58 SubChannel #59]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.184644       1 logging.go:55] [core] [Channel #22 SubChannel #416]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.184679       1 logging.go:55] [core] [Channel #49 SubChannel #50]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.184958       1 logging.go:55] [core] [Channel #25 SubChannel #415]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.184982       1 logging.go:55] [core] [Channel #97 SubChannel #98]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.184994       1 logging.go:55] [core] [Channel #130 SubChannel #131]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.185255       1 logging.go:55] [core] [Channel #127 SubChannel #128]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.185738       1 logging.go:55] [core] [Channel #94 SubChannel #95]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.185831       1 logging.go:55] [core] [Channel #67 SubChannel #68]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.186004       1 logging.go:55] [core] [Channel #85 SubChannel #86]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.186155       1 logging.go:55] [core] [Channel #157 SubChannel #158]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.186237       1 logging.go:55] [core] [Channel #106 SubChannel #107]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.186274       1 logging.go:55] [core] [Channel #37 SubChannel #38]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.186278       1 logging.go:55] [core] [Channel #121 SubChannel #122]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.186312       1 logging.go:55] [core] [Channel #160 SubChannel #161]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.186340       1 logging.go:55] [core] [Channel #133 SubChannel #134]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.186356       1 logging.go:55] [core] [Channel #112 SubChannel #113]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.186550       1 logging.go:55] [core] [Channel #17 SubChannel #18]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.186553       1 logging.go:55] [core] [Channel #103 SubChannel #104]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.186592       1 logging.go:55] [core] [Channel #148 SubChannel #149]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.186598       1 logging.go:55] [core] [Channel #124 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.186652       1 logging.go:55] [core] [Channel #154 SubChannel #155]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.186936       1 logging.go:55] [core] [Channel #34 SubChannel #35]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.186937       1 logging.go:55] [core] [Channel #109 SubChannel #110]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.186953       1 logging.go:55] [core] [Channel #118 SubChannel #119]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.189467       1 logging.go:55] [core] [Channel #163 SubChannel #164]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.189525       1 logging.go:55] [core] [Channel #28 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.189577       1 logging.go:55] [core] [Channel #40 SubChannel #41]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.189588       1 logging.go:55] [core] [Channel #79 SubChannel #80]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.189611       1 logging.go:55] [core] [Channel #46 SubChannel #47]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.189724       1 logging.go:55] [core] [Channel #145 SubChannel #146]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.189748       1 logging.go:55] [core] [Channel #31 SubChannel #32]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.189753       1 logging.go:55] [core] [Channel #178 SubChannel #179]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.189973       1 logging.go:55] [core] [Channel #166 SubChannel #167]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.190013       1 logging.go:55] [core] [Channel #73 SubChannel #74]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.190026       1 logging.go:55] [core] [Channel #172 SubChannel #173]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.190043       1 logging.go:55] [core] [Channel #64 SubChannel #65]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.190075       1 logging.go:55] [core] [Channel #76 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.190111       1 logging.go:55] [core] [Channel #100 SubChannel #101]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.192432       1 logging.go:55] [core] [Channel #142 SubChannel #143]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.192433       1 logging.go:55] [core] [Channel #7 SubChannel #8]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.192441       1 logging.go:55] [core] [Channel #151 SubChannel #152]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 18:04:59.192627       1 logging.go:55] [core] [Channel #88 SubChannel #89]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-apiserver [ce6075c0d2b6] <==
I0307 05:07:01.203300       1 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0307 05:07:01.203348       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0307 05:07:01.203386       1 local_available_controller.go:156] Starting LocalAvailability controller
I0307 05:07:01.203384       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0307 05:07:01.203300       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0307 05:07:01.207339       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0307 05:07:01.203390       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I0307 05:07:01.203401       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I0307 05:07:01.207702       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0307 05:07:01.203403       1 controller.go:119] Starting legacy_token_tracking_controller
I0307 05:07:01.207722       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0307 05:07:01.203411       1 controller.go:78] Starting OpenAPI AggregationController
I0307 05:07:01.208193       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0307 05:07:01.208618       1 cluster_authentication_trust_controller.go:462] Starting cluster_authentication_trust_controller controller
I0307 05:07:01.208635       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0307 05:07:01.203428       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0307 05:07:01.209183       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0307 05:07:01.203623       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0307 05:07:01.203927       1 controller.go:142] Starting OpenAPI controller
I0307 05:07:01.203946       1 controller.go:90] Starting OpenAPI V3 controller
I0307 05:07:01.203959       1 naming_controller.go:294] Starting NamingConditionController
I0307 05:07:01.203968       1 establishing_controller.go:81] Starting EstablishingController
I0307 05:07:01.204005       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0307 05:07:01.204010       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0307 05:07:01.204017       1 crd_finalizer.go:269] Starting CRDFinalizer
I0307 05:07:01.217454       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0307 05:07:01.217519       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0307 05:07:01.300463       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0307 05:07:01.300756       1 policy_source.go:240] refreshing policies
I0307 05:07:01.301509       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0307 05:07:01.303266       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0307 05:07:01.303295       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0307 05:07:01.303374       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0307 05:07:01.308130       1 cache.go:39] Caches are synced for LocalAvailability controller
I0307 05:07:01.308201       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0307 05:07:01.308773       1 shared_informer.go:320] Caches are synced for configmaps
I0307 05:07:01.309009       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0307 05:07:01.309258       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0307 05:07:01.309320       1 aggregator.go:171] initial CRD sync complete...
I0307 05:07:01.309333       1 autoregister_controller.go:144] Starting autoregister controller
I0307 05:07:01.309339       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0307 05:07:01.309348       1 cache.go:39] Caches are synced for autoregister controller
I0307 05:07:01.319178       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0307 05:07:01.399366       1 shared_informer.go:320] Caches are synced for node_authorizer
I0307 05:07:02.206235       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0307 05:07:03.124963       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0307 05:07:04.723149       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0307 05:07:04.807291       1 controller.go:615] quota admission added evaluator for: endpoints
I0307 05:07:04.923681       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0307 05:07:04.998171       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0307 05:58:08.176673       1 alloc.go:330] "allocated clusterIPs" service="default/webapp" clusterIPs={"IPv4":"10.108.177.162"}
I0307 06:01:30.220756       1 alloc.go:330] "allocated clusterIPs" service="default/mongo-service" clusterIPs={"IPv4":"10.97.134.164"}
I0307 06:14:20.068156       1 alloc.go:330] "allocated clusterIPs" service="default/webapp-service" clusterIPs={"IPv4":"10.97.28.173"}
I0307 06:19:20.627443       1 alloc.go:330] "allocated clusterIPs" service="default/webapp-service" clusterIPs={"IPv4":"10.101.212.96"}
I0307 06:20:44.699166       1 alloc.go:330] "allocated clusterIPs" service="default/webapp-service" clusterIPs={"IPv4":"10.105.227.103"}
I0307 06:53:55.649132       1 alloc.go:330] "allocated clusterIPs" service="default/webapp-service" clusterIPs={"IPv4":"10.107.249.21"}
I0307 07:04:53.817657       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0307 07:04:53.827754       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0307 07:06:49.405900       1 alloc.go:330] "allocated clusterIPs" service="default/mongo-service" clusterIPs={"IPv4":"10.105.197.50"}
I0307 07:06:59.768806       1 alloc.go:330] "allocated clusterIPs" service="default/webapp-service" clusterIPs={"IPv4":"10.109.239.21"}


==> kube-controller-manager [1767af9b375b] <==
I0307 06:19:20.716655       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/webapp-deployment-cf8d68b5b" duration="35.167µs"
I0307 06:19:20.738009       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/webapp-deployment-cf8d68b5b" duration="64.5µs"
I0307 06:19:20.743362       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/webapp-deployment-cf8d68b5b" duration="31.25µs"
I0307 06:19:25.565086       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/webapp-deployment-cf8d68b5b" duration="574.833µs"
I0307 06:19:27.595304       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/webapp-deployment-cf8d68b5b" duration="179.333µs"
I0307 06:20:28.302404       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/webapp-deployment-cf8d68b5b" duration="21.916µs"
I0307 06:20:44.787038       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="default/webapp-service" err="EndpointSlice informer cache is out of date"
I0307 06:20:44.795973       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/webapp-deployment-58b66bfbb9" duration="185.57ms"
I0307 06:20:44.800554       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/webapp-deployment-58b66bfbb9" duration="4.512166ms"
I0307 06:20:44.800767       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/webapp-deployment-58b66bfbb9" duration="66.916µs"
I0307 06:20:44.801291       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/webapp-deployment-58b66bfbb9" duration="17µs"
I0307 06:20:44.806728       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/webapp-deployment-58b66bfbb9" duration="24.375µs"
I0307 06:20:48.935452       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/webapp-deployment-58b66bfbb9" duration="196.208µs"
I0307 06:20:51.102580       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/webapp-deployment-58b66bfbb9" duration="681.583µs"
I0307 06:21:04.796182       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0307 06:23:52.299990       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/webapp-deployment-58b66bfbb9" duration="10.375µs"
I0307 06:26:09.987730       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0307 06:30:54.714141       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-7cc74bf478" duration="10.875µs"
I0307 06:33:55.835026       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0307 06:39:03.563089       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0307 06:44:10.203290       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0307 06:49:15.442132       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0307 06:53:31.380160       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-6cc977d86b" duration="35.223166ms"
I0307 06:53:31.385706       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-6cc977d86b" duration="5.343667ms"
I0307 06:53:31.385765       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-6cc977d86b" duration="24.708µs"
I0307 06:53:31.388526       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-6cc977d86b" duration="65.791µs"
I0307 06:53:31.397031       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-6cc977d86b" duration="52.75µs"
I0307 06:53:35.363932       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-6cc977d86b" duration="126.917µs"
I0307 06:53:37.402805       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-6cc977d86b" duration="66.583µs"
I0307 06:53:55.660771       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="default/webapp-service" err="EndpointSlice informer cache is out of date"
I0307 06:53:55.676461       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/webapp-deployment-74d458d687" duration="37.808458ms"
I0307 06:53:55.681354       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/webapp-deployment-74d458d687" duration="4.844708ms"
I0307 06:53:55.681464       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/webapp-deployment-74d458d687" duration="47.625µs"
I0307 06:53:55.682593       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/webapp-deployment-74d458d687" duration="21.917µs"
I0307 06:53:55.688049       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/webapp-deployment-74d458d687" duration="27.25µs"
I0307 06:53:58.669201       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/webapp-deployment-74d458d687" duration="40µs"
I0307 06:54:01.714458       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/webapp-deployment-74d458d687" duration="149.791µs"
I0307 06:54:22.068722       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0307 06:59:29.346487       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0307 07:04:36.309703       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0307 07:04:43.891615       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube-ffcbb5874" duration="10.083µs"
I0307 07:04:43.892062       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-6cc977d86b" duration="13.708µs"
I0307 07:04:43.892110       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/webapp-deployment-74d458d687" duration="4.459µs"
I0307 07:06:49.458122       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-74ff5b95db" duration="62.292833ms"
I0307 07:06:49.465757       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-74ff5b95db" duration="7.519333ms"
I0307 07:06:49.465841       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-74ff5b95db" duration="34.958µs"
I0307 07:06:49.471781       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-74ff5b95db" duration="25.042µs"
I0307 07:06:49.477154       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-74ff5b95db" duration="86.542µs"
I0307 07:06:53.168303       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-74ff5b95db" duration="5.634792ms"
I0307 07:06:53.168429       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-74ff5b95db" duration="68.667µs"
I0307 07:06:55.273902       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-74ff5b95db" duration="11.475958ms"
I0307 07:06:55.274718       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-deployment-74ff5b95db" duration="46.917µs"
I0307 07:06:59.859004       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/webapp-deployment-74f4657677" duration="130.47425ms"
I0307 07:06:59.868064       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/webapp-deployment-74f4657677" duration="8.9255ms"
I0307 07:06:59.868222       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/webapp-deployment-74f4657677" duration="68.667µs"
I0307 07:06:59.875099       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/webapp-deployment-74f4657677" duration="41.25µs"
I0307 07:06:59.881656       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/webapp-deployment-74f4657677" duration="32.833µs"
I0307 07:07:04.020978       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/webapp-deployment-74f4657677" duration="2.424083ms"
I0307 07:07:06.042748       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/webapp-deployment-74f4657677" duration="57.625µs"
I0307 07:09:42.398623       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-controller-manager [afcdf99b2844] <==
I0306 16:01:39.764530       1 shared_informer.go:320] Caches are synced for persistent volume
I0306 16:01:39.764689       1 shared_informer.go:320] Caches are synced for GC
I0306 16:01:39.767421       1 shared_informer.go:320] Caches are synced for taint
I0306 16:01:39.767552       1 node_lifecycle_controller.go:1234] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0306 16:01:39.767727       1 node_lifecycle_controller.go:886] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0306 16:01:39.767775       1 node_lifecycle_controller.go:1080] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0306 16:01:39.768511       1 shared_informer.go:320] Caches are synced for TTL after finished
I0306 16:01:39.770548       1 shared_informer.go:320] Caches are synced for resource quota
I0306 16:01:39.777668       1 shared_informer.go:320] Caches are synced for HPA
I0306 16:01:39.781857       1 shared_informer.go:320] Caches are synced for namespace
I0306 16:01:39.781898       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0306 16:01:39.782082       1 shared_informer.go:320] Caches are synced for stateful set
I0306 16:01:39.787690       1 shared_informer.go:320] Caches are synced for disruption
I0306 16:01:39.787849       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I0306 16:01:39.789479       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0306 16:01:39.789541       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I0306 16:01:39.789558       1 shared_informer.go:320] Caches are synced for PVC protection
I0306 16:01:39.789568       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I0306 16:01:39.789562       1 shared_informer.go:320] Caches are synced for PV protection
I0306 16:01:39.794633       1 shared_informer.go:320] Caches are synced for attach detach
I0306 16:01:39.797819       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0306 16:01:39.802623       1 shared_informer.go:320] Caches are synced for crt configmap
I0306 16:01:39.802650       1 shared_informer.go:320] Caches are synced for endpoint
I0306 16:01:39.802770       1 shared_informer.go:320] Caches are synced for expand
I0306 16:01:39.802880       1 shared_informer.go:320] Caches are synced for daemon sets
I0306 16:01:39.802909       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0306 16:01:39.802939       1 shared_informer.go:320] Caches are synced for ReplicationController
I0306 16:01:39.802990       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/hello-minikube-ffcbb5874" duration="50.625µs"
I0306 16:01:39.803025       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="14.25µs"
I0306 16:01:39.803044       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="10.833µs"
I0306 16:01:39.804251       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I0306 16:01:39.804281       1 shared_informer.go:320] Caches are synced for TTL
I0306 16:01:39.808602       1 shared_informer.go:320] Caches are synced for resource quota
I0306 16:01:39.825657       1 shared_informer.go:320] Caches are synced for garbage collector
I0306 16:01:39.831606       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0306 16:01:39.848591       1 shared_informer.go:320] Caches are synced for ephemeral
I0306 16:01:39.854329       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0306 16:01:39.854407       1 shared_informer.go:320] Caches are synced for service account
I0306 16:01:39.856476       1 shared_informer.go:320] Caches are synced for job
I0306 16:01:40.164307       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="361.35325ms"
I0306 16:01:40.164486       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="87.25µs"
I0306 16:02:09.569396       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="13.389333ms"
I0306 16:02:09.569807       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="304.125µs"
I0306 16:02:15.803329       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="79.292µs"
I0306 16:02:16.469116       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="7.831083ms"
I0306 16:02:16.469276       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="95.75µs"
I0306 16:02:29.845926       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="6.454292ms"
I0306 16:02:29.846011       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="38.292µs"
I0306 16:05:30.156215       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0306 16:10:34.924262       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0306 16:15:40.365233       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0306 16:20:47.708427       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0306 16:25:54.707622       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0306 16:31:01.238027       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0306 16:36:08.020362       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0306 16:41:13.079603       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0306 16:46:19.677920       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0306 18:04:56.496840       1 cleaner.go:175] "Cleaning CSR as it is more than approvedExpiration duration old and approved." logger="certificatesigningrequest-cleaner-controller" csr="csr-g2nsh" approvedExpiration="1h0m0s"
I0306 18:04:57.593745       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0306 18:04:57.897765       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-proxy [67039f9efb57] <==
I0306 16:01:38.652423       1 server_linux.go:66] "Using iptables proxy"
I0306 16:01:38.820572       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0306 16:01:38.820684       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0306 16:01:38.863288       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0306 16:01:38.863370       1 server_linux.go:170] "Using iptables Proxier"
I0306 16:01:38.864627       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0306 16:01:38.865256       1 server.go:497] "Version info" version="v1.32.0"
I0306 16:01:38.865274       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0306 16:01:38.869919       1 config.go:199] "Starting service config controller"
I0306 16:01:38.869945       1 config.go:329] "Starting node config controller"
I0306 16:01:38.869970       1 config.go:105] "Starting endpoint slice config controller"
I0306 16:01:38.870126       1 shared_informer.go:313] Waiting for caches to sync for service config
I0306 16:01:38.870204       1 shared_informer.go:313] Waiting for caches to sync for node config
I0306 16:01:38.870271       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0306 16:01:38.970613       1 shared_informer.go:320] Caches are synced for service config
I0306 16:01:38.970646       1 shared_informer.go:320] Caches are synced for node config
I0306 16:01:38.970702       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-proxy [cef1842c2fa5] <==
I0307 05:07:04.826308       1 server_linux.go:66] "Using iptables proxy"
I0307 05:07:05.041620       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0307 05:07:05.041728       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0307 05:07:05.101600       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0307 05:07:05.101681       1 server_linux.go:170] "Using iptables Proxier"
I0307 05:07:05.103567       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0307 05:07:05.105321       1 server.go:497] "Version info" version="v1.32.0"
I0307 05:07:05.105344       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0307 05:07:05.110061       1 config.go:199] "Starting service config controller"
I0307 05:07:05.110116       1 shared_informer.go:313] Waiting for caches to sync for service config
I0307 05:07:05.110146       1 config.go:329] "Starting node config controller"
I0307 05:07:05.110158       1 shared_informer.go:313] Waiting for caches to sync for node config
I0307 05:07:05.110376       1 config.go:105] "Starting endpoint slice config controller"
I0307 05:07:05.110568       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0307 05:07:05.211155       1 shared_informer.go:320] Caches are synced for node config
I0307 05:07:05.211199       1 shared_informer.go:320] Caches are synced for service config
I0307 05:07:05.211232       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-scheduler [68fb2d794eb8] <==
I0307 05:06:59.731759       1 serving.go:386] Generated self-signed cert in-memory
I0307 05:07:01.422045       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0307 05:07:01.422080       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0307 05:07:01.429070       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0307 05:07:01.429510       1 requestheader_controller.go:180] Starting RequestHeaderAuthRequestController
I0307 05:07:01.430204       1 shared_informer.go:313] Waiting for caches to sync for RequestHeaderAuthRequestController
I0307 05:07:01.430291       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0307 05:07:01.430322       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0307 05:07:01.430345       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0307 05:07:01.430360       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0307 05:07:01.496684       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0307 05:07:01.531096       1 shared_informer.go:320] Caches are synced for RequestHeaderAuthRequestController
I0307 05:07:01.531329       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0307 05:07:01.531726       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kube-scheduler [9f38e751b456] <==
I0306 16:01:33.796293       1 serving.go:386] Generated self-signed cert in-memory
W0306 16:01:35.370120       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0306 16:01:35.370203       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0306 16:01:35.370226       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0306 16:01:35.370238       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0306 16:01:35.476229       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0306 16:01:35.476265       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0306 16:01:35.482757       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0306 16:01:35.482877       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0306 16:01:35.483428       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0306 16:01:35.553221       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0306 16:01:35.653400       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0306 18:04:58.184830       1 configmap_cafile_content.go:226] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0306 18:04:58.186685       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
I0306 18:04:58.183613       1 tlsconfig.go:258] "Shutting down DynamicServingCertificateController"
E0306 18:04:58.295659       1 run.go:72] "command failed" err="finished without leader elect"


==> kubelet <==
Mar 07 07:07:52 minikube kubelet[1538]: E0307 07:07:52.779849    1538 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:momgo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k7bm8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-74f4657677-q2jmp_default(283aa588-da97-4787-8412-cda62ac9f900): CreateContainerConfigError: configmap \"momgo-config\" not found" logger="UnhandledError"
Mar 07 07:07:52 minikube kubelet[1538]: E0307 07:07:52.782466    1538 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"momgo-config\\\" not found\"" pod="default/webapp-deployment-74f4657677-q2jmp" podUID="283aa588-da97-4787-8412-cda62ac9f900"
Mar 07 07:07:55 minikube kubelet[1538]: E0307 07:07:55.534270    1538 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:momgo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4c5kx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-74f4657677-62p9k_default(62db92d6-8a01-49fe-a614-4ed144f88e88): CreateContainerConfigError: configmap \"momgo-config\" not found" logger="UnhandledError"
Mar 07 07:07:55 minikube kubelet[1538]: E0307 07:07:55.536600    1538 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"momgo-config\\\" not found\"" pod="default/webapp-deployment-74f4657677-62p9k" podUID="62db92d6-8a01-49fe-a614-4ed144f88e88"
Mar 07 07:08:06 minikube kubelet[1538]: E0307 07:08:06.496563    1538 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:momgo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k7bm8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-74f4657677-q2jmp_default(283aa588-da97-4787-8412-cda62ac9f900): CreateContainerConfigError: configmap \"momgo-config\" not found" logger="UnhandledError"
Mar 07 07:08:06 minikube kubelet[1538]: E0307 07:08:06.498798    1538 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"momgo-config\\\" not found\"" pod="default/webapp-deployment-74f4657677-q2jmp" podUID="283aa588-da97-4787-8412-cda62ac9f900"
Mar 07 07:08:11 minikube kubelet[1538]: E0307 07:08:11.712401    1538 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:momgo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4c5kx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-74f4657677-62p9k_default(62db92d6-8a01-49fe-a614-4ed144f88e88): CreateContainerConfigError: configmap \"momgo-config\" not found" logger="UnhandledError"
Mar 07 07:08:11 minikube kubelet[1538]: E0307 07:08:11.714530    1538 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"momgo-config\\\" not found\"" pod="default/webapp-deployment-74f4657677-62p9k" podUID="62db92d6-8a01-49fe-a614-4ed144f88e88"
Mar 07 07:08:22 minikube kubelet[1538]: E0307 07:08:22.989733    1538 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:momgo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k7bm8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-74f4657677-q2jmp_default(283aa588-da97-4787-8412-cda62ac9f900): CreateContainerConfigError: configmap \"momgo-config\" not found" logger="UnhandledError"
Mar 07 07:08:22 minikube kubelet[1538]: E0307 07:08:22.991776    1538 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"momgo-config\\\" not found\"" pod="default/webapp-deployment-74f4657677-q2jmp" podUID="283aa588-da97-4787-8412-cda62ac9f900"
Mar 07 07:08:29 minikube kubelet[1538]: E0307 07:08:29.493492    1538 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:momgo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4c5kx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-74f4657677-62p9k_default(62db92d6-8a01-49fe-a614-4ed144f88e88): CreateContainerConfigError: configmap \"momgo-config\" not found" logger="UnhandledError"
Mar 07 07:08:29 minikube kubelet[1538]: E0307 07:08:29.495264    1538 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"momgo-config\\\" not found\"" pod="default/webapp-deployment-74f4657677-62p9k" podUID="62db92d6-8a01-49fe-a614-4ed144f88e88"
Mar 07 07:08:40 minikube kubelet[1538]: E0307 07:08:40.710448    1538 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:momgo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k7bm8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-74f4657677-q2jmp_default(283aa588-da97-4787-8412-cda62ac9f900): CreateContainerConfigError: configmap \"momgo-config\" not found" logger="UnhandledError"
Mar 07 07:08:40 minikube kubelet[1538]: E0307 07:08:40.712356    1538 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"momgo-config\\\" not found\"" pod="default/webapp-deployment-74f4657677-q2jmp" podUID="283aa588-da97-4787-8412-cda62ac9f900"
Mar 07 07:08:46 minikube kubelet[1538]: E0307 07:08:46.729781    1538 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:momgo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4c5kx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-74f4657677-62p9k_default(62db92d6-8a01-49fe-a614-4ed144f88e88): CreateContainerConfigError: configmap \"momgo-config\" not found" logger="UnhandledError"
Mar 07 07:08:46 minikube kubelet[1538]: E0307 07:08:46.732159    1538 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"momgo-config\\\" not found\"" pod="default/webapp-deployment-74f4657677-62p9k" podUID="62db92d6-8a01-49fe-a614-4ed144f88e88"
Mar 07 07:08:57 minikube kubelet[1538]: E0307 07:08:57.760319    1538 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:momgo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k7bm8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-74f4657677-q2jmp_default(283aa588-da97-4787-8412-cda62ac9f900): CreateContainerConfigError: configmap \"momgo-config\" not found" logger="UnhandledError"
Mar 07 07:08:57 minikube kubelet[1538]: E0307 07:08:57.762551    1538 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"momgo-config\\\" not found\"" pod="default/webapp-deployment-74f4657677-q2jmp" podUID="283aa588-da97-4787-8412-cda62ac9f900"
Mar 07 07:09:03 minikube kubelet[1538]: E0307 07:09:03.779086    1538 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:momgo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4c5kx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-74f4657677-62p9k_default(62db92d6-8a01-49fe-a614-4ed144f88e88): CreateContainerConfigError: configmap \"momgo-config\" not found" logger="UnhandledError"
Mar 07 07:09:03 minikube kubelet[1538]: E0307 07:09:03.781127    1538 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"momgo-config\\\" not found\"" pod="default/webapp-deployment-74f4657677-62p9k" podUID="62db92d6-8a01-49fe-a614-4ed144f88e88"
Mar 07 07:09:10 minikube kubelet[1538]: E0307 07:09:10.596474    1538 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:momgo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k7bm8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-74f4657677-q2jmp_default(283aa588-da97-4787-8412-cda62ac9f900): CreateContainerConfigError: configmap \"momgo-config\" not found" logger="UnhandledError"
Mar 07 07:09:10 minikube kubelet[1538]: E0307 07:09:10.598489    1538 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"momgo-config\\\" not found\"" pod="default/webapp-deployment-74f4657677-q2jmp" podUID="283aa588-da97-4787-8412-cda62ac9f900"
Mar 07 07:09:17 minikube kubelet[1538]: E0307 07:09:17.556801    1538 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:momgo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4c5kx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-74f4657677-62p9k_default(62db92d6-8a01-49fe-a614-4ed144f88e88): CreateContainerConfigError: configmap \"momgo-config\" not found" logger="UnhandledError"
Mar 07 07:09:17 minikube kubelet[1538]: E0307 07:09:17.558495    1538 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"momgo-config\\\" not found\"" pod="default/webapp-deployment-74f4657677-62p9k" podUID="62db92d6-8a01-49fe-a614-4ed144f88e88"
Mar 07 07:09:23 minikube kubelet[1538]: E0307 07:09:23.860072    1538 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:momgo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k7bm8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-74f4657677-q2jmp_default(283aa588-da97-4787-8412-cda62ac9f900): CreateContainerConfigError: configmap \"momgo-config\" not found" logger="UnhandledError"
Mar 07 07:09:23 minikube kubelet[1538]: E0307 07:09:23.862490    1538 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"momgo-config\\\" not found\"" pod="default/webapp-deployment-74f4657677-q2jmp" podUID="283aa588-da97-4787-8412-cda62ac9f900"
Mar 07 07:09:33 minikube kubelet[1538]: E0307 07:09:33.639287    1538 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:momgo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4c5kx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-74f4657677-62p9k_default(62db92d6-8a01-49fe-a614-4ed144f88e88): CreateContainerConfigError: configmap \"momgo-config\" not found" logger="UnhandledError"
Mar 07 07:09:33 minikube kubelet[1538]: E0307 07:09:33.641936    1538 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"momgo-config\\\" not found\"" pod="default/webapp-deployment-74f4657677-62p9k" podUID="62db92d6-8a01-49fe-a614-4ed144f88e88"
Mar 07 07:09:37 minikube kubelet[1538]: E0307 07:09:37.719423    1538 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:momgo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k7bm8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-74f4657677-q2jmp_default(283aa588-da97-4787-8412-cda62ac9f900): CreateContainerConfigError: configmap \"momgo-config\" not found" logger="UnhandledError"
Mar 07 07:09:37 minikube kubelet[1538]: E0307 07:09:37.721419    1538 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"momgo-config\\\" not found\"" pod="default/webapp-deployment-74f4657677-q2jmp" podUID="283aa588-da97-4787-8412-cda62ac9f900"
Mar 07 07:09:49 minikube kubelet[1538]: E0307 07:09:49.096975    1538 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:momgo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4c5kx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-74f4657677-62p9k_default(62db92d6-8a01-49fe-a614-4ed144f88e88): CreateContainerConfigError: configmap \"momgo-config\" not found" logger="UnhandledError"
Mar 07 07:09:49 minikube kubelet[1538]: E0307 07:09:49.098647    1538 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"momgo-config\\\" not found\"" pod="default/webapp-deployment-74f4657677-62p9k" podUID="62db92d6-8a01-49fe-a614-4ed144f88e88"
Mar 07 07:09:54 minikube kubelet[1538]: E0307 07:09:54.689535    1538 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:momgo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k7bm8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-74f4657677-q2jmp_default(283aa588-da97-4787-8412-cda62ac9f900): CreateContainerConfigError: configmap \"momgo-config\" not found" logger="UnhandledError"
Mar 07 07:09:54 minikube kubelet[1538]: E0307 07:09:54.692002    1538 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"momgo-config\\\" not found\"" pod="default/webapp-deployment-74f4657677-q2jmp" podUID="283aa588-da97-4787-8412-cda62ac9f900"
Mar 07 07:10:06 minikube kubelet[1538]: E0307 07:10:06.714669    1538 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:momgo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4c5kx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-74f4657677-62p9k_default(62db92d6-8a01-49fe-a614-4ed144f88e88): CreateContainerConfigError: configmap \"momgo-config\" not found" logger="UnhandledError"
Mar 07 07:10:06 minikube kubelet[1538]: E0307 07:10:06.717036    1538 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"momgo-config\\\" not found\"" pod="default/webapp-deployment-74f4657677-62p9k" podUID="62db92d6-8a01-49fe-a614-4ed144f88e88"
Mar 07 07:10:11 minikube kubelet[1538]: E0307 07:10:11.414311    1538 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:momgo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k7bm8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-74f4657677-q2jmp_default(283aa588-da97-4787-8412-cda62ac9f900): CreateContainerConfigError: configmap \"momgo-config\" not found" logger="UnhandledError"
Mar 07 07:10:11 minikube kubelet[1538]: E0307 07:10:11.416123    1538 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"momgo-config\\\" not found\"" pod="default/webapp-deployment-74f4657677-q2jmp" podUID="283aa588-da97-4787-8412-cda62ac9f900"
Mar 07 07:10:22 minikube kubelet[1538]: E0307 07:10:22.803371    1538 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:momgo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4c5kx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-74f4657677-62p9k_default(62db92d6-8a01-49fe-a614-4ed144f88e88): CreateContainerConfigError: configmap \"momgo-config\" not found" logger="UnhandledError"
Mar 07 07:10:22 minikube kubelet[1538]: E0307 07:10:22.804973    1538 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"momgo-config\\\" not found\"" pod="default/webapp-deployment-74f4657677-62p9k" podUID="62db92d6-8a01-49fe-a614-4ed144f88e88"
Mar 07 07:10:27 minikube kubelet[1538]: E0307 07:10:27.915956    1538 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:momgo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k7bm8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-74f4657677-q2jmp_default(283aa588-da97-4787-8412-cda62ac9f900): CreateContainerConfigError: configmap \"momgo-config\" not found" logger="UnhandledError"
Mar 07 07:10:27 minikube kubelet[1538]: E0307 07:10:27.918491    1538 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"momgo-config\\\" not found\"" pod="default/webapp-deployment-74f4657677-q2jmp" podUID="283aa588-da97-4787-8412-cda62ac9f900"
Mar 07 07:10:37 minikube kubelet[1538]: E0307 07:10:37.780306    1538 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:momgo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4c5kx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-74f4657677-62p9k_default(62db92d6-8a01-49fe-a614-4ed144f88e88): CreateContainerConfigError: configmap \"momgo-config\" not found" logger="UnhandledError"
Mar 07 07:10:37 minikube kubelet[1538]: E0307 07:10:37.781588    1538 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"momgo-config\\\" not found\"" pod="default/webapp-deployment-74f4657677-62p9k" podUID="62db92d6-8a01-49fe-a614-4ed144f88e88"
Mar 07 07:10:46 minikube kubelet[1538]: E0307 07:10:46.464765    1538 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:momgo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k7bm8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-74f4657677-q2jmp_default(283aa588-da97-4787-8412-cda62ac9f900): CreateContainerConfigError: configmap \"momgo-config\" not found" logger="UnhandledError"
Mar 07 07:10:46 minikube kubelet[1538]: E0307 07:10:46.466416    1538 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"momgo-config\\\" not found\"" pod="default/webapp-deployment-74f4657677-q2jmp" podUID="283aa588-da97-4787-8412-cda62ac9f900"
Mar 07 07:10:50 minikube kubelet[1538]: E0307 07:10:50.615617    1538 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:momgo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4c5kx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-74f4657677-62p9k_default(62db92d6-8a01-49fe-a614-4ed144f88e88): CreateContainerConfigError: configmap \"momgo-config\" not found" logger="UnhandledError"
Mar 07 07:10:50 minikube kubelet[1538]: E0307 07:10:50.617495    1538 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"momgo-config\\\" not found\"" pod="default/webapp-deployment-74f4657677-62p9k" podUID="62db92d6-8a01-49fe-a614-4ed144f88e88"
Mar 07 07:11:00 minikube kubelet[1538]: E0307 07:11:00.577115    1538 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:momgo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k7bm8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-74f4657677-q2jmp_default(283aa588-da97-4787-8412-cda62ac9f900): CreateContainerConfigError: configmap \"momgo-config\" not found" logger="UnhandledError"
Mar 07 07:11:00 minikube kubelet[1538]: E0307 07:11:00.578946    1538 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"momgo-config\\\" not found\"" pod="default/webapp-deployment-74f4657677-q2jmp" podUID="283aa588-da97-4787-8412-cda62ac9f900"
Mar 07 07:11:06 minikube kubelet[1538]: E0307 07:11:06.483469    1538 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:momgo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4c5kx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-74f4657677-62p9k_default(62db92d6-8a01-49fe-a614-4ed144f88e88): CreateContainerConfigError: configmap \"momgo-config\" not found" logger="UnhandledError"
Mar 07 07:11:06 minikube kubelet[1538]: E0307 07:11:06.485040    1538 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"momgo-config\\\" not found\"" pod="default/webapp-deployment-74f4657677-62p9k" podUID="62db92d6-8a01-49fe-a614-4ed144f88e88"
Mar 07 07:11:16 minikube kubelet[1538]: E0307 07:11:16.614605    1538 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:momgo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k7bm8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-74f4657677-q2jmp_default(283aa588-da97-4787-8412-cda62ac9f900): CreateContainerConfigError: configmap \"momgo-config\" not found" logger="UnhandledError"
Mar 07 07:11:16 minikube kubelet[1538]: E0307 07:11:16.615911    1538 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"momgo-config\\\" not found\"" pod="default/webapp-deployment-74f4657677-q2jmp" podUID="283aa588-da97-4787-8412-cda62ac9f900"
Mar 07 07:11:24 minikube kubelet[1538]: E0307 07:11:24.866557    1538 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:momgo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4c5kx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-74f4657677-62p9k_default(62db92d6-8a01-49fe-a614-4ed144f88e88): CreateContainerConfigError: configmap \"momgo-config\" not found" logger="UnhandledError"
Mar 07 07:11:24 minikube kubelet[1538]: E0307 07:11:24.868280    1538 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"momgo-config\\\" not found\"" pod="default/webapp-deployment-74f4657677-62p9k" podUID="62db92d6-8a01-49fe-a614-4ed144f88e88"
Mar 07 07:11:32 minikube kubelet[1538]: E0307 07:11:32.510868    1538 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:momgo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k7bm8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-74f4657677-q2jmp_default(283aa588-da97-4787-8412-cda62ac9f900): CreateContainerConfigError: configmap \"momgo-config\" not found" logger="UnhandledError"
Mar 07 07:11:32 minikube kubelet[1538]: E0307 07:11:32.513342    1538 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"momgo-config\\\" not found\"" pod="default/webapp-deployment-74f4657677-q2jmp" podUID="283aa588-da97-4787-8412-cda62ac9f900"
Mar 07 07:11:39 minikube kubelet[1538]: E0307 07:11:39.653260    1538 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-user,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongo-secret,},Key:mongo-password,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:momgo-config,},Key:mongo-url,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4c5kx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod webapp-deployment-74f4657677-62p9k_default(62db92d6-8a01-49fe-a614-4ed144f88e88): CreateContainerConfigError: configmap \"momgo-config\" not found" logger="UnhandledError"
Mar 07 07:11:39 minikube kubelet[1538]: E0307 07:11:39.655485    1538 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"momgo-config\\\" not found\"" pod="default/webapp-deployment-74f4657677-62p9k" podUID="62db92d6-8a01-49fe-a614-4ed144f88e88"


==> kubernetes-dashboard [0891f1f98c6e] <==
2025/03/07 05:07:58 Using namespace: kubernetes-dashboard
2025/03/07 05:07:58 Using in-cluster config to connect to apiserver
2025/03/07 05:07:58 Using secret token for csrf signing
2025/03/07 05:07:58 Initializing csrf token from kubernetes-dashboard-csrf secret
2025/03/07 05:07:58 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2025/03/07 05:07:58 Successful initial request to the apiserver, version: v1.32.0
2025/03/07 05:07:58 Generating JWE encryption key
2025/03/07 05:07:58 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2025/03/07 05:07:58 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2025/03/07 05:07:58 Initializing JWE encryption key from synchronized object
2025/03/07 05:07:58 Creating in-cluster Sidecar client
2025/03/07 05:07:58 Serving insecurely on HTTP port: 9090
2025/03/07 05:07:58 Successful request to sidecar
2025/03/07 05:07:58 Starting overwatch


==> kubernetes-dashboard [7e6e0d13db3b] <==
2025/03/07 05:07:04 Using namespace: kubernetes-dashboard
2025/03/07 05:07:04 Using in-cluster config to connect to apiserver
2025/03/07 05:07:04 Using secret token for csrf signing
2025/03/07 05:07:04 Initializing csrf token from kubernetes-dashboard-csrf secret
2025/03/07 05:07:04 Starting overwatch
panic: Get "https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/secrets/kubernetes-dashboard-csrf": dial tcp 10.96.0.1:443: i/o timeout

goroutine 1 [running]:
github.com/kubernetes/dashboard/src/app/backend/client/csrf.(*csrfTokenManager).init(0x400055fa78)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:41 +0x2c0
github.com/kubernetes/dashboard/src/app/backend/client/csrf.NewCsrfTokenManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:66
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).initCSRFKey(0x4000410180)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:527 +0x7c
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).init(0x14957f0?)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:495 +0x30
github.com/kubernetes/dashboard/src/app/backend/client.NewClientManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:594
main.main()
	/home/runner/work/dashboard/dashboard/src/app/backend/dashboard.go:96 +0x1bc


==> storage-provisioner [182d2a35e1f5] <==
I0307 05:07:46.132854       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0307 05:07:46.151310       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0307 05:07:46.151847       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0307 05:08:03.588943       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0307 05:08:03.589306       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_47c488a5-4e5c-49cc-95fd-30cdfaaa5ec3!
I0307 05:08:03.589856       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"c03af8ab-ae7d-4b23-8ae0-2870b31cd981", APIVersion:"v1", ResourceVersion:"3459", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_47c488a5-4e5c-49cc-95fd-30cdfaaa5ec3 became leader
I0307 05:08:03.691623       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_47c488a5-4e5c-49cc-95fd-30cdfaaa5ec3!


==> storage-provisioner [a6a54303feed] <==
I0307 05:07:04.226451       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0307 05:07:34.240808       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

